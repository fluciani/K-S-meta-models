{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cbd1a70-107e-4492-8d14-c9cc118b6474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17f8edc2-2fc6-4158-8436-893202f53223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Policy</th>\n",
       "      <th>yG</th>\n",
       "      <th>yIn</th>\n",
       "      <th>yPi</th>\n",
       "      <th>yU</th>\n",
       "      <th>r</th>\n",
       "      <th>U-8</th>\n",
       "      <th>Y-8</th>\n",
       "      <th>Yr-8</th>\n",
       "      <th>C-8</th>\n",
       "      <th>...</th>\n",
       "      <th>U</th>\n",
       "      <th>Y</th>\n",
       "      <th>Growth</th>\n",
       "      <th>CPI</th>\n",
       "      <th>In</th>\n",
       "      <th>InR</th>\n",
       "      <th>Pi</th>\n",
       "      <th>PiR</th>\n",
       "      <th>Deb</th>\n",
       "      <th>Gini</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1country-p0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>1767613.342</td>\n",
       "      <td>1.260673e+06</td>\n",
       "      <td>1.459991e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0838</td>\n",
       "      <td>2027485.652</td>\n",
       "      <td>0.065972</td>\n",
       "      <td>1.720362</td>\n",
       "      <td>1727950.758</td>\n",
       "      <td>1.004411e+06</td>\n",
       "      <td>305991.9582</td>\n",
       "      <td>177864.84890</td>\n",
       "      <td>3.557405</td>\n",
       "      <td>0.197288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1country-p0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0552</td>\n",
       "      <td>2159344.291</td>\n",
       "      <td>1.475312e+06</td>\n",
       "      <td>1.771117e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0634</td>\n",
       "      <td>2340870.703</td>\n",
       "      <td>0.048124</td>\n",
       "      <td>1.749143</td>\n",
       "      <td>1995892.418</td>\n",
       "      <td>1.141069e+06</td>\n",
       "      <td>363755.1360</td>\n",
       "      <td>207961.94840</td>\n",
       "      <td>3.274980</td>\n",
       "      <td>0.204952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1country-p0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.1378</td>\n",
       "      <td>1525747.371</td>\n",
       "      <td>1.063048e+06</td>\n",
       "      <td>1.341039e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0452</td>\n",
       "      <td>1950752.861</td>\n",
       "      <td>0.144288</td>\n",
       "      <td>1.776093</td>\n",
       "      <td>1636470.543</td>\n",
       "      <td>9.213881e+05</td>\n",
       "      <td>303377.1019</td>\n",
       "      <td>170811.54020</td>\n",
       "      <td>3.776751</td>\n",
       "      <td>0.185090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1country-p0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>1095513.541</td>\n",
       "      <td>7.961295e+05</td>\n",
       "      <td>9.520544e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>1409756.470</td>\n",
       "      <td>0.139152</td>\n",
       "      <td>1.644339</td>\n",
       "      <td>1202242.417</td>\n",
       "      <td>7.311401e+05</td>\n",
       "      <td>202841.5703</td>\n",
       "      <td>123357.48150</td>\n",
       "      <td>5.532918</td>\n",
       "      <td>0.176241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1country-p0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.1434</td>\n",
       "      <td>1595656.175</td>\n",
       "      <td>1.140445e+06</td>\n",
       "      <td>1.377761e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0692</td>\n",
       "      <td>1942844.669</td>\n",
       "      <td>0.121321</td>\n",
       "      <td>1.708837</td>\n",
       "      <td>1652359.387</td>\n",
       "      <td>9.669497e+05</td>\n",
       "      <td>293176.3673</td>\n",
       "      <td>171564.85880</td>\n",
       "      <td>4.105299</td>\n",
       "      <td>0.191467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>1country-p4029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>1361683.054</td>\n",
       "      <td>1.013075e+06</td>\n",
       "      <td>1.135110e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>1655061.042</td>\n",
       "      <td>0.107446</td>\n",
       "      <td>1.682220</td>\n",
       "      <td>1427873.465</td>\n",
       "      <td>8.488032e+05</td>\n",
       "      <td>200066.5452</td>\n",
       "      <td>118930.09170</td>\n",
       "      <td>3.950405</td>\n",
       "      <td>0.163205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>1country-p4029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0746</td>\n",
       "      <td>1347193.985</td>\n",
       "      <td>9.536083e+05</td>\n",
       "      <td>1.139672e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0858</td>\n",
       "      <td>1483908.276</td>\n",
       "      <td>0.071059</td>\n",
       "      <td>1.668065</td>\n",
       "      <td>1326621.048</td>\n",
       "      <td>7.953053e+05</td>\n",
       "      <td>159462.0191</td>\n",
       "      <td>95596.99881</td>\n",
       "      <td>4.189787</td>\n",
       "      <td>0.180427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>1country-p4029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.1112</td>\n",
       "      <td>1303774.004</td>\n",
       "      <td>9.798066e+05</td>\n",
       "      <td>1.096790e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>1584578.465</td>\n",
       "      <td>0.092193</td>\n",
       "      <td>1.646522</td>\n",
       "      <td>1360677.869</td>\n",
       "      <td>8.263952e+05</td>\n",
       "      <td>201465.4217</td>\n",
       "      <td>122358.17010</td>\n",
       "      <td>4.129867</td>\n",
       "      <td>0.165975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>1country-p4029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0642</td>\n",
       "      <td>1789660.534</td>\n",
       "      <td>1.274180e+06</td>\n",
       "      <td>1.475860e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0386</td>\n",
       "      <td>2158542.435</td>\n",
       "      <td>0.103182</td>\n",
       "      <td>1.733324</td>\n",
       "      <td>1854238.298</td>\n",
       "      <td>1.069759e+06</td>\n",
       "      <td>280984.7781</td>\n",
       "      <td>162107.47370</td>\n",
       "      <td>3.155506</td>\n",
       "      <td>0.145088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>1country-p4029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0734</td>\n",
       "      <td>1626001.690</td>\n",
       "      <td>1.176384e+06</td>\n",
       "      <td>1.390213e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>2026407.911</td>\n",
       "      <td>0.128083</td>\n",
       "      <td>1.757292</td>\n",
       "      <td>1722438.885</td>\n",
       "      <td>9.801669e+05</td>\n",
       "      <td>267244.8383</td>\n",
       "      <td>152077.69350</td>\n",
       "      <td>2.553132</td>\n",
       "      <td>0.158460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows Ã— 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Policy   yG  yIn   yPi    yU       r     U-8          Y-8  \\\n",
       "0         1country-p0  0.0  0.0  0.00  0.00  0.0050  0.0700  1767613.342   \n",
       "1         1country-p0  0.0  0.0  0.00  0.00  0.0050  0.0552  2159344.291   \n",
       "2         1country-p0  0.0  0.0  0.00  0.00  0.0050  0.1378  1525747.371   \n",
       "3         1country-p0  0.0  0.0  0.00  0.00  0.0050  0.2250  1095513.541   \n",
       "4         1country-p0  0.0  0.0  0.00  0.00  0.0050  0.1434  1595656.175   \n",
       "...               ...  ...  ...   ...   ...     ...     ...          ...   \n",
       "29995  1country-p4029  0.0  0.0  0.01  0.01  0.0125  0.0584  1361683.054   \n",
       "29996  1country-p4029  0.0  0.0  0.01  0.01  0.0125  0.0746  1347193.985   \n",
       "29997  1country-p4029  0.0  0.0  0.01  0.01  0.0125  0.1112  1303774.004   \n",
       "29998  1country-p4029  0.0  0.0  0.01  0.01  0.0125  0.0642  1789660.534   \n",
       "29999  1country-p4029  0.0  0.0  0.01  0.01  0.0125  0.0734  1626001.690   \n",
       "\n",
       "               Yr-8           C-8  ...       U            Y    Growth  \\\n",
       "0      1.260673e+06  1.459991e+06  ...  0.0838  2027485.652  0.065972   \n",
       "1      1.475312e+06  1.771117e+06  ...  0.0634  2340870.703  0.048124   \n",
       "2      1.063048e+06  1.341039e+06  ...  0.0452  1950752.861  0.144288   \n",
       "3      7.961295e+05  9.520544e+05  ...  0.0842  1409756.470  0.139152   \n",
       "4      1.140445e+06  1.377761e+06  ...  0.0692  1942844.669  0.121321   \n",
       "...             ...           ...  ...     ...          ...       ...   \n",
       "29995  1.013075e+06  1.135110e+06  ...  0.0450  1655061.042  0.107446   \n",
       "29996  9.536083e+05  1.139672e+06  ...  0.0858  1483908.276  0.071059   \n",
       "29997  9.798066e+05  1.096790e+06  ...  0.0370  1584578.465  0.092193   \n",
       "29998  1.274180e+06  1.475860e+06  ...  0.0386  2158542.435  0.103182   \n",
       "29999  1.176384e+06  1.390213e+06  ...  0.0338  2026407.911  0.128083   \n",
       "\n",
       "            CPI           In           InR           Pi           PiR  \\\n",
       "0      1.720362  1727950.758  1.004411e+06  305991.9582  177864.84890   \n",
       "1      1.749143  1995892.418  1.141069e+06  363755.1360  207961.94840   \n",
       "2      1.776093  1636470.543  9.213881e+05  303377.1019  170811.54020   \n",
       "3      1.644339  1202242.417  7.311401e+05  202841.5703  123357.48150   \n",
       "4      1.708837  1652359.387  9.669497e+05  293176.3673  171564.85880   \n",
       "...         ...          ...           ...          ...           ...   \n",
       "29995  1.682220  1427873.465  8.488032e+05  200066.5452  118930.09170   \n",
       "29996  1.668065  1326621.048  7.953053e+05  159462.0191   95596.99881   \n",
       "29997  1.646522  1360677.869  8.263952e+05  201465.4217  122358.17010   \n",
       "29998  1.733324  1854238.298  1.069759e+06  280984.7781  162107.47370   \n",
       "29999  1.757292  1722438.885  9.801669e+05  267244.8383  152077.69350   \n",
       "\n",
       "            Deb      Gini  \n",
       "0      3.557405  0.197288  \n",
       "1      3.274980  0.204952  \n",
       "2      3.776751  0.185090  \n",
       "3      5.532918  0.176241  \n",
       "4      4.105299  0.191467  \n",
       "...         ...       ...  \n",
       "29995  3.950405  0.163205  \n",
       "29996  4.189787  0.180427  \n",
       "29997  4.129867  0.165975  \n",
       "29998  3.155506  0.145088  \n",
       "29999  2.553132  0.158460  \n",
       "\n",
       "[30000 rows x 168 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/User/Desktop/Python/Data30.csv', sep=\";\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2629e869-6965-4a69-8710-0060dc59d141",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:-10].values\n",
    "y = df['Pi'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e06c6eac-645f-4320-893a-e55b4ddecf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = r'C:\\Users\\User\\Desktop\\Python\\PyTorch\\Modelli\\CPI\\Configurazione\\best_config.txt'\n",
    "weights_path = r'C:\\Users\\User\\Desktop\\Python\\PyTorch\\Modelli\\CPI\\Pesi\\best_weights.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feb1fe4a-0ae6-4648-829b-539665390ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X2, y_train, y2 = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X2, y2, test_size=0.5, random_state=1)\n",
    "\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
    "X_val = scaler.transform(X_val).astype(np.float32)\n",
    "X_test = scaler.transform(X_test).astype(np.float32)\n",
    "y_train = y_train.astype(np.float32).reshape(-1, 1)\n",
    "y_val = y_val.astype(np.float32).reshape(-1, 1)\n",
    "y_test = y_test.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "bs = 256\n",
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "val_dataset = TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe199484-a285-4164-99dc-832f8bbd9576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network class\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size=1, dropout_rate=0):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        layers = []\n",
    "        in_features = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_features, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            in_features = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(in_features, output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a267c981-f23e-4b1b-ba4c-8829301ff94b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 6 layers and 50 neurons per layer...\n",
      "Epoch 1/160, Validation Loss: 0.001536\n",
      "Epoch 2/160, Validation Loss: 0.000907\n",
      "Epoch 3/160, Validation Loss: 0.001488\n",
      "Epoch 4/160, Validation Loss: 0.000587\n",
      "Epoch 5/160, Validation Loss: 0.000892\n",
      "Epoch 6/160, Validation Loss: 0.000515\n",
      "Epoch 7/160, Validation Loss: 0.001756\n",
      "Epoch 8/160, Validation Loss: 0.000961\n",
      "Epoch 9/160, Validation Loss: 0.000389\n",
      "Epoch 10/160, Validation Loss: 0.000402\n",
      "Epoch 11/160, Validation Loss: 0.000382\n",
      "Epoch 12/160, Validation Loss: 0.000336\n",
      "Epoch 13/160, Validation Loss: 0.000882\n",
      "Epoch 14/160, Validation Loss: 0.000326\n",
      "Epoch 15/160, Validation Loss: 0.000466\n",
      "Epoch 16/160, Validation Loss: 0.001442\n",
      "Epoch 17/160, Validation Loss: 0.002452\n",
      "Epoch 18/160, Validation Loss: 0.000305\n",
      "Epoch 19/160, Validation Loss: 0.000444\n",
      "Epoch 20/160, Validation Loss: 0.000596\n",
      "Epoch 21/160, Validation Loss: 0.000350\n",
      "Epoch 22/160, Validation Loss: 0.000900\n",
      "Epoch 23/160, Validation Loss: 0.000416\n",
      "Epoch 24/160, Validation Loss: 0.000544\n",
      "Epoch 25/160, Validation Loss: 0.000263\n",
      "Epoch 26/160, Validation Loss: 0.001040\n",
      "Epoch 27/160, Validation Loss: 0.000333\n",
      "Epoch 28/160, Validation Loss: 0.000381\n",
      "Epoch 29/160, Validation Loss: 0.000332\n",
      "Epoch 30/160, Validation Loss: 0.000289\n",
      "Epoch 31/160, Validation Loss: 0.000279\n",
      "Epoch 32/160, Validation Loss: 0.000214\n",
      "Epoch 33/160, Validation Loss: 0.000799\n",
      "Epoch 34/160, Validation Loss: 0.000513\n",
      "Epoch 35/160, Validation Loss: 0.000245\n",
      "Epoch 36/160, Validation Loss: 0.000293\n",
      "Epoch 37/160, Validation Loss: 0.000223\n",
      "Epoch 38/160, Validation Loss: 0.000230\n",
      "Epoch 39/160, Validation Loss: 0.000289\n",
      "Epoch 40/160, Validation Loss: 0.001162\n",
      "Epoch 41/160, Validation Loss: 0.000346\n",
      "Epoch 42/160, Validation Loss: 0.000384\n",
      "Epoch 43/160, Validation Loss: 0.000228\n",
      "Epoch 44/160, Validation Loss: 0.000412\n",
      "Epoch 45/160, Validation Loss: 0.000224\n",
      "Epoch 46/160, Validation Loss: 0.000296\n",
      "Epoch 47/160, Validation Loss: 0.000628\n",
      "Epoch 48/160, Validation Loss: 0.000904\n",
      "Epoch 49/160, Validation Loss: 0.000220\n",
      "Epoch 50/160, Validation Loss: 0.000197\n",
      "Epoch 51/160, Validation Loss: 0.000193\n",
      "Epoch 52/160, Validation Loss: 0.000841\n",
      "Epoch 53/160, Validation Loss: 0.000270\n",
      "Epoch 54/160, Validation Loss: 0.000514\n",
      "Epoch 55/160, Validation Loss: 0.000209\n",
      "Epoch 56/160, Validation Loss: 0.000314\n",
      "Epoch 57/160, Validation Loss: 0.000429\n",
      "Epoch 58/160, Validation Loss: 0.000855\n",
      "Epoch 59/160, Validation Loss: 0.000188\n",
      "Epoch 60/160, Validation Loss: 0.000253\n",
      "Epoch 61/160, Validation Loss: 0.000206\n",
      "Epoch 62/160, Validation Loss: 0.000188\n",
      "Epoch 63/160, Validation Loss: 0.000330\n",
      "Epoch 64/160, Validation Loss: 0.000730\n",
      "Epoch 65/160, Validation Loss: 0.000453\n",
      "Epoch 66/160, Validation Loss: 0.000809\n",
      "Epoch 67/160, Validation Loss: 0.000427\n",
      "Epoch 68/160, Validation Loss: 0.000248\n",
      "Epoch 69/160, Validation Loss: 0.000200\n",
      "Epoch 70/160, Validation Loss: 0.000205\n",
      "Epoch 71/160, Validation Loss: 0.000313\n",
      "Epoch 72/160, Validation Loss: 0.000199\n",
      "Epoch 73/160, Validation Loss: 0.000568\n",
      "Epoch 74/160, Validation Loss: 0.000196\n",
      "Epoch 75/160, Validation Loss: 0.000374\n",
      "Epoch 76/160, Validation Loss: 0.000371\n",
      "Epoch 77/160, Validation Loss: 0.000319\n",
      "Epoch 78/160, Validation Loss: 0.000300\n",
      "Epoch 79/160, Validation Loss: 0.000198\n",
      "Epoch 80/160, Validation Loss: 0.000493\n",
      "Epoch 81/160, Validation Loss: 0.000437\n",
      "Epoch 82/160, Validation Loss: 0.000185\n",
      "Epoch 83/160, Validation Loss: 0.000191\n",
      "Epoch 84/160, Validation Loss: 0.000189\n",
      "Epoch 85/160, Validation Loss: 0.000647\n",
      "Epoch 86/160, Validation Loss: 0.000303\n",
      "Epoch 87/160, Validation Loss: 0.000209\n",
      "Epoch 88/160, Validation Loss: 0.000188\n",
      "Epoch 89/160, Validation Loss: 0.000180\n",
      "Epoch 90/160, Validation Loss: 0.000376\n",
      "Epoch 91/160, Validation Loss: 0.000174\n",
      "Epoch 92/160, Validation Loss: 0.000172\n",
      "Epoch 93/160, Validation Loss: 0.000237\n",
      "Epoch 94/160, Validation Loss: 0.000214\n",
      "Epoch 95/160, Validation Loss: 0.000230\n",
      "Epoch 96/160, Validation Loss: 0.000798\n",
      "Epoch 97/160, Validation Loss: 0.000214\n",
      "Epoch 98/160, Validation Loss: 0.000270\n",
      "Epoch 99/160, Validation Loss: 0.000241\n",
      "Epoch 100/160, Validation Loss: 0.000393\n",
      "Epoch 101/160, Validation Loss: 0.000314\n",
      "Epoch 102/160, Validation Loss: 0.000192\n",
      "Epoch 103/160, Validation Loss: 0.000183\n",
      "Epoch 104/160, Validation Loss: 0.001003\n",
      "Epoch 105/160, Validation Loss: 0.000221\n",
      "Epoch 106/160, Validation Loss: 0.000283\n",
      "Epoch 107/160, Validation Loss: 0.000469\n",
      "Epoch 108/160, Validation Loss: 0.000217\n",
      "Epoch 109/160, Validation Loss: 0.000256\n",
      "Epoch 110/160, Validation Loss: 0.000205\n",
      "Epoch 111/160, Validation Loss: 0.000265\n",
      "Epoch 112/160, Validation Loss: 0.000178\n",
      "Epoch 113/160, Validation Loss: 0.000980\n",
      "Epoch 114/160, Validation Loss: 0.000184\n",
      "Epoch 115/160, Validation Loss: 0.000310\n",
      "Epoch 116/160, Validation Loss: 0.000322\n",
      "Epoch 117/160, Validation Loss: 0.000245\n",
      "Epoch 118/160, Validation Loss: 0.000181\n",
      "Epoch 119/160, Validation Loss: 0.000225\n",
      "Epoch 120/160, Validation Loss: 0.000281\n",
      "Epoch 121/160, Validation Loss: 0.000195\n",
      "Epoch 122/160, Validation Loss: 0.000183\n",
      "Epoch 123/160, Validation Loss: 0.000474\n",
      "Epoch 124/160, Validation Loss: 0.000279\n",
      "Epoch 125/160, Validation Loss: 0.000193\n",
      "Epoch 126/160, Validation Loss: 0.000203\n",
      "Epoch 127/160, Validation Loss: 0.000159\n",
      "Epoch 128/160, Validation Loss: 0.000251\n",
      "Epoch 129/160, Validation Loss: 0.000221\n",
      "Epoch 130/160, Validation Loss: 0.000205\n",
      "Epoch 131/160, Validation Loss: 0.000205\n",
      "Epoch 132/160, Validation Loss: 0.000262\n",
      "Epoch 133/160, Validation Loss: 0.000198\n",
      "Epoch 134/160, Validation Loss: 0.000285\n",
      "Epoch 135/160, Validation Loss: 0.000743\n",
      "Epoch 136/160, Validation Loss: 0.000691\n",
      "Epoch 137/160, Validation Loss: 0.000164\n",
      "Epoch 138/160, Validation Loss: 0.000195\n",
      "Epoch 139/160, Validation Loss: 0.000163\n",
      "Epoch 140/160, Validation Loss: 0.000192\n",
      "Epoch 141/160, Validation Loss: 0.000177\n",
      "Epoch 142/160, Validation Loss: 0.000431\n",
      "Epoch 143/160, Validation Loss: 0.000213\n",
      "Epoch 144/160, Validation Loss: 0.000182\n",
      "Epoch 145/160, Validation Loss: 0.000332\n",
      "Epoch 146/160, Validation Loss: 0.000379\n",
      "Epoch 147/160, Validation Loss: 0.000175\n",
      "Epoch 148/160, Validation Loss: 0.000459\n",
      "Epoch 149/160, Validation Loss: 0.000688\n",
      "Epoch 150/160, Validation Loss: 0.000169\n",
      "Epoch 151/160, Validation Loss: 0.000359\n",
      "Epoch 152/160, Validation Loss: 0.000163\n",
      "Epoch 153/160, Validation Loss: 0.000315\n",
      "Epoch 154/160, Validation Loss: 0.000236\n",
      "Epoch 155/160, Validation Loss: 0.000200\n",
      "Epoch 156/160, Validation Loss: 0.000307\n",
      "Epoch 157/160, Validation Loss: 0.000249\n",
      "Epoch 158/160, Validation Loss: 0.000892\n",
      "Epoch 159/160, Validation Loss: 0.000185\n",
      "Epoch 160/160, Validation Loss: 0.000207\n",
      "Training model with 6 layers and 60 neurons per layer...\n",
      "Epoch 1/160, Validation Loss: 0.001951\n",
      "Epoch 2/160, Validation Loss: 0.000633\n",
      "Epoch 3/160, Validation Loss: 0.000457\n",
      "Epoch 4/160, Validation Loss: 0.000533\n",
      "Epoch 5/160, Validation Loss: 0.000670\n",
      "Epoch 6/160, Validation Loss: 0.000727\n",
      "Epoch 7/160, Validation Loss: 0.000310\n",
      "Epoch 8/160, Validation Loss: 0.000604\n",
      "Epoch 9/160, Validation Loss: 0.000306\n",
      "Epoch 10/160, Validation Loss: 0.000312\n",
      "Epoch 11/160, Validation Loss: 0.000421\n",
      "Epoch 12/160, Validation Loss: 0.000255\n",
      "Epoch 13/160, Validation Loss: 0.000894\n",
      "Epoch 14/160, Validation Loss: 0.000245\n",
      "Epoch 15/160, Validation Loss: 0.000341\n",
      "Epoch 16/160, Validation Loss: 0.000799\n",
      "Epoch 17/160, Validation Loss: 0.000254\n",
      "Epoch 18/160, Validation Loss: 0.000758\n",
      "Epoch 19/160, Validation Loss: 0.000238\n",
      "Epoch 20/160, Validation Loss: 0.000234\n",
      "Epoch 21/160, Validation Loss: 0.000221\n",
      "Epoch 22/160, Validation Loss: 0.000235\n",
      "Epoch 23/160, Validation Loss: 0.000306\n",
      "Epoch 24/160, Validation Loss: 0.000275\n",
      "Epoch 25/160, Validation Loss: 0.000214\n",
      "Epoch 26/160, Validation Loss: 0.000218\n",
      "Epoch 27/160, Validation Loss: 0.000209\n",
      "Epoch 28/160, Validation Loss: 0.000230\n",
      "Epoch 29/160, Validation Loss: 0.000215\n",
      "Epoch 30/160, Validation Loss: 0.000377\n",
      "Epoch 31/160, Validation Loss: 0.000256\n",
      "Epoch 32/160, Validation Loss: 0.000334\n",
      "Epoch 33/160, Validation Loss: 0.001739\n",
      "Epoch 34/160, Validation Loss: 0.000515\n",
      "Epoch 35/160, Validation Loss: 0.000966\n",
      "Epoch 36/160, Validation Loss: 0.000368\n",
      "Epoch 37/160, Validation Loss: 0.000205\n",
      "Epoch 38/160, Validation Loss: 0.000298\n",
      "Epoch 39/160, Validation Loss: 0.000215\n",
      "Epoch 40/160, Validation Loss: 0.000230\n",
      "Epoch 41/160, Validation Loss: 0.000290\n",
      "Epoch 42/160, Validation Loss: 0.000477\n",
      "Epoch 43/160, Validation Loss: 0.000327\n",
      "Epoch 44/160, Validation Loss: 0.000229\n",
      "Epoch 45/160, Validation Loss: 0.000811\n",
      "Epoch 46/160, Validation Loss: 0.000462\n",
      "Epoch 47/160, Validation Loss: 0.000237\n",
      "Epoch 48/160, Validation Loss: 0.000423\n",
      "Epoch 49/160, Validation Loss: 0.000219\n",
      "Epoch 50/160, Validation Loss: 0.000242\n",
      "Epoch 51/160, Validation Loss: 0.000202\n",
      "Epoch 52/160, Validation Loss: 0.000225\n",
      "Epoch 53/160, Validation Loss: 0.000192\n",
      "Epoch 54/160, Validation Loss: 0.000359\n",
      "Epoch 55/160, Validation Loss: 0.000175\n",
      "Epoch 56/160, Validation Loss: 0.000203\n",
      "Epoch 57/160, Validation Loss: 0.000228\n",
      "Epoch 58/160, Validation Loss: 0.000234\n",
      "Epoch 59/160, Validation Loss: 0.000197\n",
      "Epoch 60/160, Validation Loss: 0.000246\n",
      "Epoch 61/160, Validation Loss: 0.000177\n",
      "Epoch 62/160, Validation Loss: 0.000271\n",
      "Epoch 63/160, Validation Loss: 0.000250\n",
      "Epoch 64/160, Validation Loss: 0.000201\n",
      "Epoch 65/160, Validation Loss: 0.000744\n",
      "Epoch 66/160, Validation Loss: 0.000221\n",
      "Epoch 67/160, Validation Loss: 0.001033\n",
      "Epoch 68/160, Validation Loss: 0.000460\n",
      "Epoch 69/160, Validation Loss: 0.000508\n",
      "Epoch 70/160, Validation Loss: 0.000492\n",
      "Epoch 71/160, Validation Loss: 0.000213\n",
      "Epoch 72/160, Validation Loss: 0.000235\n",
      "Epoch 73/160, Validation Loss: 0.000238\n",
      "Epoch 74/160, Validation Loss: 0.000199\n",
      "Epoch 75/160, Validation Loss: 0.000296\n",
      "Epoch 76/160, Validation Loss: 0.000987\n",
      "Epoch 77/160, Validation Loss: 0.000500\n",
      "Epoch 78/160, Validation Loss: 0.000168\n",
      "Epoch 79/160, Validation Loss: 0.000390\n",
      "Epoch 80/160, Validation Loss: 0.000278\n",
      "Epoch 81/160, Validation Loss: 0.000400\n",
      "Epoch 82/160, Validation Loss: 0.000213\n",
      "Epoch 83/160, Validation Loss: 0.000593\n",
      "Epoch 84/160, Validation Loss: 0.000520\n",
      "Epoch 85/160, Validation Loss: 0.000217\n",
      "Epoch 86/160, Validation Loss: 0.001485\n",
      "Epoch 87/160, Validation Loss: 0.000361\n",
      "Epoch 88/160, Validation Loss: 0.000172\n",
      "Epoch 89/160, Validation Loss: 0.000196\n",
      "Epoch 90/160, Validation Loss: 0.000241\n",
      "Epoch 91/160, Validation Loss: 0.000580\n",
      "Epoch 92/160, Validation Loss: 0.000205\n",
      "Epoch 93/160, Validation Loss: 0.000272\n",
      "Epoch 94/160, Validation Loss: 0.000258\n",
      "Epoch 95/160, Validation Loss: 0.000684\n",
      "Epoch 96/160, Validation Loss: 0.000440\n",
      "Epoch 97/160, Validation Loss: 0.000160\n",
      "Epoch 98/160, Validation Loss: 0.000242\n",
      "Epoch 99/160, Validation Loss: 0.000169\n",
      "Epoch 100/160, Validation Loss: 0.000733\n",
      "Epoch 101/160, Validation Loss: 0.000283\n",
      "Epoch 102/160, Validation Loss: 0.000215\n",
      "Epoch 103/160, Validation Loss: 0.000198\n",
      "Epoch 104/160, Validation Loss: 0.002384\n",
      "Epoch 105/160, Validation Loss: 0.000350\n",
      "Epoch 106/160, Validation Loss: 0.000941\n",
      "Epoch 107/160, Validation Loss: 0.000270\n",
      "Epoch 108/160, Validation Loss: 0.000368\n",
      "Epoch 109/160, Validation Loss: 0.000193\n",
      "Epoch 110/160, Validation Loss: 0.000218\n",
      "Epoch 111/160, Validation Loss: 0.000164\n",
      "Epoch 112/160, Validation Loss: 0.000282\n",
      "Epoch 113/160, Validation Loss: 0.000215\n",
      "Epoch 114/160, Validation Loss: 0.000313\n",
      "Epoch 115/160, Validation Loss: 0.000399\n",
      "Epoch 116/160, Validation Loss: 0.000360\n",
      "Epoch 117/160, Validation Loss: 0.000201\n",
      "Epoch 118/160, Validation Loss: 0.000259\n",
      "Epoch 119/160, Validation Loss: 0.000189\n",
      "Epoch 120/160, Validation Loss: 0.000478\n",
      "Epoch 121/160, Validation Loss: 0.000180\n",
      "Epoch 122/160, Validation Loss: 0.000164\n",
      "Epoch 123/160, Validation Loss: 0.000194\n",
      "Epoch 124/160, Validation Loss: 0.000208\n",
      "Epoch 125/160, Validation Loss: 0.000218\n",
      "Epoch 126/160, Validation Loss: 0.000229\n",
      "Epoch 127/160, Validation Loss: 0.000163\n",
      "Epoch 128/160, Validation Loss: 0.000269\n",
      "Epoch 129/160, Validation Loss: 0.000182\n",
      "Epoch 130/160, Validation Loss: 0.000438\n",
      "Epoch 131/160, Validation Loss: 0.000177\n",
      "Epoch 132/160, Validation Loss: 0.000177\n",
      "Epoch 133/160, Validation Loss: 0.000169\n",
      "Epoch 134/160, Validation Loss: 0.000566\n",
      "Epoch 135/160, Validation Loss: 0.000298\n",
      "Epoch 136/160, Validation Loss: 0.000587\n",
      "Epoch 137/160, Validation Loss: 0.000374\n",
      "Epoch 138/160, Validation Loss: 0.000165\n",
      "Epoch 139/160, Validation Loss: 0.000165\n",
      "Epoch 140/160, Validation Loss: 0.000230\n",
      "Epoch 141/160, Validation Loss: 0.000199\n",
      "Epoch 142/160, Validation Loss: 0.000273\n",
      "Epoch 143/160, Validation Loss: 0.000308\n",
      "Epoch 144/160, Validation Loss: 0.000219\n",
      "Epoch 145/160, Validation Loss: 0.001464\n",
      "Epoch 146/160, Validation Loss: 0.000379\n",
      "Epoch 147/160, Validation Loss: 0.000167\n",
      "Epoch 148/160, Validation Loss: 0.000157\n",
      "Epoch 149/160, Validation Loss: 0.001203\n",
      "Epoch 150/160, Validation Loss: 0.000207\n",
      "Epoch 151/160, Validation Loss: 0.000222\n",
      "Epoch 152/160, Validation Loss: 0.000252\n",
      "Epoch 153/160, Validation Loss: 0.000758\n",
      "Epoch 154/160, Validation Loss: 0.000719\n",
      "Epoch 155/160, Validation Loss: 0.000305\n",
      "Epoch 156/160, Validation Loss: 0.000464\n",
      "Epoch 157/160, Validation Loss: 0.000297\n",
      "Epoch 158/160, Validation Loss: 0.000343\n",
      "Epoch 159/160, Validation Loss: 0.000244\n",
      "Epoch 160/160, Validation Loss: 0.000254\n",
      "Training model with 6 layers and 70 neurons per layer...\n",
      "Epoch 1/160, Validation Loss: 0.001625\n",
      "Epoch 2/160, Validation Loss: 0.000620\n",
      "Epoch 3/160, Validation Loss: 0.000500\n",
      "Epoch 4/160, Validation Loss: 0.000496\n",
      "Epoch 5/160, Validation Loss: 0.000530\n",
      "Epoch 6/160, Validation Loss: 0.000479\n",
      "Epoch 7/160, Validation Loss: 0.000332\n",
      "Epoch 8/160, Validation Loss: 0.000354\n",
      "Epoch 9/160, Validation Loss: 0.000484\n",
      "Epoch 10/160, Validation Loss: 0.000762\n",
      "Epoch 11/160, Validation Loss: 0.000274\n",
      "Epoch 12/160, Validation Loss: 0.000261\n",
      "Epoch 13/160, Validation Loss: 0.000451\n",
      "Epoch 14/160, Validation Loss: 0.000242\n",
      "Epoch 15/160, Validation Loss: 0.001127\n",
      "Epoch 16/160, Validation Loss: 0.000243\n",
      "Epoch 17/160, Validation Loss: 0.000785\n",
      "Epoch 18/160, Validation Loss: 0.000318\n",
      "Epoch 19/160, Validation Loss: 0.000373\n",
      "Epoch 20/160, Validation Loss: 0.001602\n",
      "Epoch 21/160, Validation Loss: 0.000208\n",
      "Epoch 22/160, Validation Loss: 0.000215\n",
      "Epoch 23/160, Validation Loss: 0.000289\n",
      "Epoch 24/160, Validation Loss: 0.000269\n",
      "Epoch 25/160, Validation Loss: 0.000234\n",
      "Epoch 26/160, Validation Loss: 0.000211\n",
      "Epoch 27/160, Validation Loss: 0.000682\n",
      "Epoch 28/160, Validation Loss: 0.000612\n",
      "Epoch 29/160, Validation Loss: 0.000336\n",
      "Epoch 30/160, Validation Loss: 0.000251\n",
      "Epoch 31/160, Validation Loss: 0.000402\n",
      "Epoch 32/160, Validation Loss: 0.000212\n",
      "Epoch 33/160, Validation Loss: 0.000190\n",
      "Epoch 34/160, Validation Loss: 0.000374\n",
      "Epoch 35/160, Validation Loss: 0.000187\n",
      "Epoch 36/160, Validation Loss: 0.000851\n",
      "Epoch 37/160, Validation Loss: 0.000224\n",
      "Epoch 38/160, Validation Loss: 0.000496\n",
      "Epoch 39/160, Validation Loss: 0.000981\n",
      "Epoch 40/160, Validation Loss: 0.000237\n",
      "Epoch 41/160, Validation Loss: 0.000194\n",
      "Epoch 42/160, Validation Loss: 0.001642\n",
      "Epoch 43/160, Validation Loss: 0.000375\n",
      "Epoch 44/160, Validation Loss: 0.000309\n",
      "Epoch 45/160, Validation Loss: 0.000199\n",
      "Epoch 46/160, Validation Loss: 0.000213\n",
      "Epoch 47/160, Validation Loss: 0.000743\n",
      "Epoch 48/160, Validation Loss: 0.000172\n",
      "Epoch 49/160, Validation Loss: 0.000293\n",
      "Epoch 50/160, Validation Loss: 0.000204\n",
      "Epoch 51/160, Validation Loss: 0.000502\n",
      "Epoch 52/160, Validation Loss: 0.000421\n",
      "Epoch 53/160, Validation Loss: 0.000174\n",
      "Epoch 54/160, Validation Loss: 0.000206\n",
      "Epoch 55/160, Validation Loss: 0.000518\n",
      "Epoch 56/160, Validation Loss: 0.000256\n",
      "Epoch 57/160, Validation Loss: 0.000231\n",
      "Epoch 58/160, Validation Loss: 0.000173\n",
      "Epoch 59/160, Validation Loss: 0.001112\n",
      "Epoch 60/160, Validation Loss: 0.000250\n",
      "Epoch 61/160, Validation Loss: 0.000763\n",
      "Epoch 62/160, Validation Loss: 0.000199\n",
      "Epoch 63/160, Validation Loss: 0.001029\n",
      "Epoch 64/160, Validation Loss: 0.000667\n",
      "Epoch 65/160, Validation Loss: 0.000216\n",
      "Epoch 66/160, Validation Loss: 0.000175\n",
      "Epoch 67/160, Validation Loss: 0.000200\n",
      "Epoch 68/160, Validation Loss: 0.000260\n",
      "Epoch 69/160, Validation Loss: 0.000293\n",
      "Epoch 70/160, Validation Loss: 0.000256\n",
      "Epoch 71/160, Validation Loss: 0.000493\n",
      "Epoch 72/160, Validation Loss: 0.000312\n",
      "Epoch 73/160, Validation Loss: 0.000478\n",
      "Epoch 74/160, Validation Loss: 0.000203\n",
      "Epoch 75/160, Validation Loss: 0.000275\n",
      "Epoch 76/160, Validation Loss: 0.000303\n",
      "Epoch 77/160, Validation Loss: 0.000192\n",
      "Epoch 78/160, Validation Loss: 0.000265\n",
      "Epoch 79/160, Validation Loss: 0.000936\n",
      "Epoch 80/160, Validation Loss: 0.002535\n",
      "Epoch 81/160, Validation Loss: 0.000251\n",
      "Epoch 82/160, Validation Loss: 0.000383\n",
      "Epoch 83/160, Validation Loss: 0.000282\n",
      "Epoch 84/160, Validation Loss: 0.000260\n",
      "Epoch 85/160, Validation Loss: 0.000343\n",
      "Epoch 86/160, Validation Loss: 0.000270\n",
      "Epoch 87/160, Validation Loss: 0.000179\n",
      "Epoch 88/160, Validation Loss: 0.000592\n",
      "Epoch 89/160, Validation Loss: 0.000172\n",
      "Epoch 90/160, Validation Loss: 0.000444\n",
      "Epoch 91/160, Validation Loss: 0.000457\n",
      "Epoch 92/160, Validation Loss: 0.000296\n",
      "Epoch 93/160, Validation Loss: 0.000383\n",
      "Epoch 94/160, Validation Loss: 0.000184\n",
      "Epoch 95/160, Validation Loss: 0.000233\n",
      "Epoch 96/160, Validation Loss: 0.000267\n",
      "Epoch 97/160, Validation Loss: 0.000516\n",
      "Epoch 98/160, Validation Loss: 0.000315\n",
      "Epoch 99/160, Validation Loss: 0.000246\n",
      "Epoch 100/160, Validation Loss: 0.000214\n",
      "Epoch 101/160, Validation Loss: 0.000191\n",
      "Epoch 102/160, Validation Loss: 0.000414\n",
      "Epoch 103/160, Validation Loss: 0.000292\n",
      "Epoch 104/160, Validation Loss: 0.000169\n",
      "Epoch 105/160, Validation Loss: 0.000194\n",
      "Epoch 106/160, Validation Loss: 0.000270\n",
      "Epoch 107/160, Validation Loss: 0.000311\n",
      "Epoch 108/160, Validation Loss: 0.000248\n",
      "Epoch 109/160, Validation Loss: 0.000331\n",
      "Epoch 110/160, Validation Loss: 0.000236\n",
      "Epoch 111/160, Validation Loss: 0.000181\n",
      "Epoch 112/160, Validation Loss: 0.000188\n",
      "Epoch 113/160, Validation Loss: 0.000199\n",
      "Epoch 114/160, Validation Loss: 0.000174\n",
      "Epoch 115/160, Validation Loss: 0.000271\n",
      "Epoch 116/160, Validation Loss: 0.000316\n",
      "Epoch 117/160, Validation Loss: 0.000405\n",
      "Epoch 118/160, Validation Loss: 0.000185\n",
      "Epoch 119/160, Validation Loss: 0.000175\n",
      "Epoch 120/160, Validation Loss: 0.000227\n",
      "Epoch 121/160, Validation Loss: 0.000319\n",
      "Epoch 122/160, Validation Loss: 0.000528\n",
      "Epoch 123/160, Validation Loss: 0.000230\n",
      "Epoch 124/160, Validation Loss: 0.000227\n",
      "Epoch 125/160, Validation Loss: 0.000210\n",
      "Epoch 126/160, Validation Loss: 0.000265\n",
      "Epoch 127/160, Validation Loss: 0.000422\n",
      "Epoch 128/160, Validation Loss: 0.000477\n",
      "Epoch 129/160, Validation Loss: 0.000179\n",
      "Epoch 130/160, Validation Loss: 0.000348\n",
      "Epoch 131/160, Validation Loss: 0.000388\n",
      "Epoch 132/160, Validation Loss: 0.000202\n",
      "Epoch 133/160, Validation Loss: 0.000177\n",
      "Epoch 134/160, Validation Loss: 0.000312\n",
      "Epoch 135/160, Validation Loss: 0.000514\n",
      "Epoch 136/160, Validation Loss: 0.000186\n",
      "Epoch 137/160, Validation Loss: 0.000179\n",
      "Epoch 138/160, Validation Loss: 0.000335\n",
      "Epoch 139/160, Validation Loss: 0.000173\n",
      "Epoch 140/160, Validation Loss: 0.000174\n",
      "Epoch 141/160, Validation Loss: 0.000170\n",
      "Epoch 142/160, Validation Loss: 0.000445\n",
      "Epoch 143/160, Validation Loss: 0.000170\n",
      "Epoch 144/160, Validation Loss: 0.000435\n",
      "Epoch 145/160, Validation Loss: 0.000413\n",
      "Epoch 146/160, Validation Loss: 0.000640\n",
      "Epoch 147/160, Validation Loss: 0.000165\n",
      "Epoch 148/160, Validation Loss: 0.000228\n",
      "Epoch 149/160, Validation Loss: 0.000171\n",
      "Epoch 150/160, Validation Loss: 0.000195\n",
      "Epoch 151/160, Validation Loss: 0.000172\n",
      "Epoch 152/160, Validation Loss: 0.000463\n",
      "Epoch 153/160, Validation Loss: 0.000212\n",
      "Epoch 154/160, Validation Loss: 0.000178\n",
      "Epoch 155/160, Validation Loss: 0.000379\n",
      "Epoch 156/160, Validation Loss: 0.000182\n",
      "Epoch 157/160, Validation Loss: 0.000241\n",
      "Epoch 158/160, Validation Loss: 0.000176\n",
      "Epoch 159/160, Validation Loss: 0.000166\n",
      "Epoch 160/160, Validation Loss: 0.000222\n",
      "Training model with 7 layers and 50 neurons per layer...\n",
      "Epoch 1/160, Validation Loss: 0.001452\n",
      "Epoch 2/160, Validation Loss: 0.000689\n",
      "Epoch 3/160, Validation Loss: 0.000587\n",
      "Epoch 4/160, Validation Loss: 0.000576\n",
      "Epoch 5/160, Validation Loss: 0.000420\n",
      "Epoch 6/160, Validation Loss: 0.000397\n",
      "Epoch 7/160, Validation Loss: 0.000361\n",
      "Epoch 8/160, Validation Loss: 0.000416\n",
      "Epoch 9/160, Validation Loss: 0.000441\n",
      "Epoch 10/160, Validation Loss: 0.000534\n",
      "Epoch 11/160, Validation Loss: 0.000506\n",
      "Epoch 12/160, Validation Loss: 0.000412\n",
      "Epoch 13/160, Validation Loss: 0.000347\n",
      "Epoch 14/160, Validation Loss: 0.000303\n",
      "Epoch 15/160, Validation Loss: 0.000301\n",
      "Epoch 16/160, Validation Loss: 0.000310\n",
      "Epoch 17/160, Validation Loss: 0.000503\n",
      "Epoch 18/160, Validation Loss: 0.000283\n",
      "Epoch 19/160, Validation Loss: 0.000893\n",
      "Epoch 20/160, Validation Loss: 0.000298\n",
      "Epoch 21/160, Validation Loss: 0.000241\n",
      "Epoch 22/160, Validation Loss: 0.000314\n",
      "Epoch 23/160, Validation Loss: 0.000242\n",
      "Epoch 24/160, Validation Loss: 0.000313\n",
      "Epoch 25/160, Validation Loss: 0.000590\n",
      "Epoch 26/160, Validation Loss: 0.000231\n",
      "Epoch 27/160, Validation Loss: 0.000252\n",
      "Epoch 28/160, Validation Loss: 0.000417\n",
      "Epoch 29/160, Validation Loss: 0.000223\n",
      "Epoch 30/160, Validation Loss: 0.000390\n",
      "Epoch 31/160, Validation Loss: 0.000246\n",
      "Epoch 32/160, Validation Loss: 0.000206\n",
      "Epoch 33/160, Validation Loss: 0.000262\n",
      "Epoch 34/160, Validation Loss: 0.000425\n",
      "Epoch 35/160, Validation Loss: 0.000209\n",
      "Epoch 36/160, Validation Loss: 0.000208\n",
      "Epoch 37/160, Validation Loss: 0.000547\n",
      "Epoch 38/160, Validation Loss: 0.000194\n",
      "Epoch 39/160, Validation Loss: 0.001012\n",
      "Epoch 40/160, Validation Loss: 0.000254\n",
      "Epoch 41/160, Validation Loss: 0.000207\n",
      "Epoch 42/160, Validation Loss: 0.000375\n",
      "Epoch 43/160, Validation Loss: 0.000221\n",
      "Epoch 44/160, Validation Loss: 0.000816\n",
      "Epoch 45/160, Validation Loss: 0.000224\n",
      "Epoch 46/160, Validation Loss: 0.000440\n",
      "Epoch 47/160, Validation Loss: 0.000474\n",
      "Epoch 48/160, Validation Loss: 0.000186\n",
      "Epoch 49/160, Validation Loss: 0.000524\n",
      "Epoch 50/160, Validation Loss: 0.000243\n",
      "Epoch 51/160, Validation Loss: 0.000219\n",
      "Epoch 52/160, Validation Loss: 0.000177\n",
      "Epoch 53/160, Validation Loss: 0.000243\n",
      "Epoch 54/160, Validation Loss: 0.000666\n",
      "Epoch 55/160, Validation Loss: 0.000391\n",
      "Epoch 56/160, Validation Loss: 0.007378\n",
      "Epoch 57/160, Validation Loss: 0.000398\n",
      "Epoch 58/160, Validation Loss: 0.000228\n",
      "Epoch 59/160, Validation Loss: 0.000209\n",
      "Epoch 60/160, Validation Loss: 0.000194\n",
      "Epoch 61/160, Validation Loss: 0.000204\n",
      "Epoch 62/160, Validation Loss: 0.000211\n",
      "Epoch 63/160, Validation Loss: 0.000221\n",
      "Epoch 64/160, Validation Loss: 0.000232\n",
      "Epoch 65/160, Validation Loss: 0.000794\n",
      "Epoch 66/160, Validation Loss: 0.000219\n",
      "Epoch 67/160, Validation Loss: 0.000393\n",
      "Epoch 68/160, Validation Loss: 0.000213\n",
      "Epoch 69/160, Validation Loss: 0.000243\n",
      "Epoch 70/160, Validation Loss: 0.000914\n",
      "Epoch 71/160, Validation Loss: 0.000449\n",
      "Epoch 72/160, Validation Loss: 0.000425\n",
      "Epoch 73/160, Validation Loss: 0.000191\n",
      "Epoch 74/160, Validation Loss: 0.000451\n",
      "Epoch 75/160, Validation Loss: 0.000170\n",
      "Epoch 76/160, Validation Loss: 0.000224\n",
      "Epoch 77/160, Validation Loss: 0.000727\n",
      "Epoch 78/160, Validation Loss: 0.000759\n",
      "Epoch 79/160, Validation Loss: 0.000229\n",
      "Epoch 80/160, Validation Loss: 0.000301\n",
      "Epoch 81/160, Validation Loss: 0.001269\n",
      "Epoch 82/160, Validation Loss: 0.000218\n",
      "Epoch 83/160, Validation Loss: 0.000596\n",
      "Epoch 84/160, Validation Loss: 0.000282\n",
      "Epoch 85/160, Validation Loss: 0.000355\n",
      "Epoch 86/160, Validation Loss: 0.000172\n",
      "Epoch 87/160, Validation Loss: 0.000177\n",
      "Epoch 88/160, Validation Loss: 0.000617\n",
      "Epoch 89/160, Validation Loss: 0.000476\n",
      "Epoch 90/160, Validation Loss: 0.000252\n",
      "Epoch 91/160, Validation Loss: 0.000726\n",
      "Epoch 92/160, Validation Loss: 0.000853\n",
      "Epoch 93/160, Validation Loss: 0.000733\n",
      "Epoch 94/160, Validation Loss: 0.000387\n",
      "Epoch 95/160, Validation Loss: 0.000200\n",
      "Epoch 96/160, Validation Loss: 0.000192\n",
      "Epoch 97/160, Validation Loss: 0.000484\n",
      "Epoch 98/160, Validation Loss: 0.000677\n",
      "Epoch 99/160, Validation Loss: 0.000616\n",
      "Epoch 100/160, Validation Loss: 0.000179\n",
      "Epoch 101/160, Validation Loss: 0.000270\n",
      "Epoch 102/160, Validation Loss: 0.000167\n",
      "Epoch 103/160, Validation Loss: 0.000192\n",
      "Epoch 104/160, Validation Loss: 0.000814\n",
      "Epoch 105/160, Validation Loss: 0.000253\n",
      "Epoch 106/160, Validation Loss: 0.000638\n",
      "Epoch 107/160, Validation Loss: 0.000175\n",
      "Epoch 108/160, Validation Loss: 0.000167\n",
      "Epoch 109/160, Validation Loss: 0.000329\n",
      "Epoch 110/160, Validation Loss: 0.000381\n",
      "Epoch 111/160, Validation Loss: 0.000253\n",
      "Epoch 112/160, Validation Loss: 0.000188\n",
      "Epoch 113/160, Validation Loss: 0.000640\n",
      "Epoch 114/160, Validation Loss: 0.000191\n",
      "Epoch 115/160, Validation Loss: 0.000366\n",
      "Epoch 116/160, Validation Loss: 0.000619\n",
      "Epoch 117/160, Validation Loss: 0.000202\n",
      "Epoch 118/160, Validation Loss: 0.000493\n",
      "Epoch 119/160, Validation Loss: 0.000203\n",
      "Epoch 120/160, Validation Loss: 0.000236\n",
      "Epoch 121/160, Validation Loss: 0.000294\n",
      "Epoch 122/160, Validation Loss: 0.000488\n",
      "Epoch 123/160, Validation Loss: 0.000165\n",
      "Epoch 124/160, Validation Loss: 0.000250\n",
      "Epoch 125/160, Validation Loss: 0.000231\n",
      "Epoch 126/160, Validation Loss: 0.000161\n",
      "Epoch 127/160, Validation Loss: 0.000329\n",
      "Epoch 128/160, Validation Loss: 0.000181\n",
      "Epoch 129/160, Validation Loss: 0.000568\n",
      "Epoch 130/160, Validation Loss: 0.000169\n",
      "Epoch 131/160, Validation Loss: 0.000666\n",
      "Epoch 132/160, Validation Loss: 0.000162\n",
      "Epoch 133/160, Validation Loss: 0.000361\n",
      "Epoch 134/160, Validation Loss: 0.000172\n",
      "Epoch 135/160, Validation Loss: 0.000569\n",
      "Epoch 136/160, Validation Loss: 0.000673\n",
      "Epoch 137/160, Validation Loss: 0.000198\n",
      "Epoch 138/160, Validation Loss: 0.000356\n",
      "Epoch 139/160, Validation Loss: 0.000195\n",
      "Epoch 140/160, Validation Loss: 0.000201\n",
      "Epoch 141/160, Validation Loss: 0.000753\n",
      "Epoch 142/160, Validation Loss: 0.000194\n",
      "Epoch 143/160, Validation Loss: 0.000318\n",
      "Epoch 144/160, Validation Loss: 0.000216\n",
      "Epoch 145/160, Validation Loss: 0.000209\n",
      "Epoch 146/160, Validation Loss: 0.000189\n",
      "Epoch 147/160, Validation Loss: 0.000497\n",
      "Epoch 148/160, Validation Loss: 0.000368\n",
      "Epoch 149/160, Validation Loss: 0.000272\n",
      "Epoch 150/160, Validation Loss: 0.000266\n",
      "Epoch 151/160, Validation Loss: 0.000198\n",
      "Epoch 152/160, Validation Loss: 0.000260\n",
      "Epoch 153/160, Validation Loss: 0.000322\n",
      "Epoch 154/160, Validation Loss: 0.000163\n",
      "Epoch 155/160, Validation Loss: 0.000210\n",
      "Epoch 156/160, Validation Loss: 0.000190\n",
      "Epoch 157/160, Validation Loss: 0.000171\n",
      "Epoch 158/160, Validation Loss: 0.000227\n",
      "Epoch 159/160, Validation Loss: 0.000217\n",
      "Epoch 160/160, Validation Loss: 0.000180\n",
      "Training model with 7 layers and 60 neurons per layer...\n",
      "Epoch 1/160, Validation Loss: 0.001062\n",
      "Epoch 2/160, Validation Loss: 0.000685\n",
      "Epoch 3/160, Validation Loss: 0.000673\n",
      "Epoch 4/160, Validation Loss: 0.001896\n",
      "Epoch 5/160, Validation Loss: 0.000773\n",
      "Epoch 6/160, Validation Loss: 0.000466\n",
      "Epoch 7/160, Validation Loss: 0.000429\n",
      "Epoch 8/160, Validation Loss: 0.000375\n",
      "Epoch 9/160, Validation Loss: 0.000325\n",
      "Epoch 10/160, Validation Loss: 0.000308\n",
      "Epoch 11/160, Validation Loss: 0.000291\n",
      "Epoch 12/160, Validation Loss: 0.000302\n",
      "Epoch 13/160, Validation Loss: 0.000578\n",
      "Epoch 14/160, Validation Loss: 0.000844\n",
      "Epoch 15/160, Validation Loss: 0.000274\n",
      "Epoch 16/160, Validation Loss: 0.000276\n",
      "Epoch 17/160, Validation Loss: 0.000386\n",
      "Epoch 18/160, Validation Loss: 0.000258\n",
      "Epoch 19/160, Validation Loss: 0.000391\n",
      "Epoch 20/160, Validation Loss: 0.000258\n",
      "Epoch 21/160, Validation Loss: 0.000886\n",
      "Epoch 22/160, Validation Loss: 0.000213\n",
      "Epoch 23/160, Validation Loss: 0.000249\n",
      "Epoch 24/160, Validation Loss: 0.000230\n",
      "Epoch 25/160, Validation Loss: 0.000951\n",
      "Epoch 26/160, Validation Loss: 0.000317\n",
      "Epoch 27/160, Validation Loss: 0.000496\n",
      "Epoch 28/160, Validation Loss: 0.000485\n",
      "Epoch 29/160, Validation Loss: 0.000401\n",
      "Epoch 30/160, Validation Loss: 0.000382\n",
      "Epoch 31/160, Validation Loss: 0.000507\n",
      "Epoch 32/160, Validation Loss: 0.000604\n",
      "Epoch 33/160, Validation Loss: 0.000239\n",
      "Epoch 34/160, Validation Loss: 0.001040\n",
      "Epoch 35/160, Validation Loss: 0.000386\n",
      "Epoch 36/160, Validation Loss: 0.000567\n",
      "Epoch 37/160, Validation Loss: 0.000234\n",
      "Epoch 38/160, Validation Loss: 0.000347\n",
      "Epoch 39/160, Validation Loss: 0.000342\n",
      "Epoch 40/160, Validation Loss: 0.001679\n",
      "Epoch 41/160, Validation Loss: 0.000345\n",
      "Epoch 42/160, Validation Loss: 0.000386\n",
      "Epoch 43/160, Validation Loss: 0.001210\n",
      "Epoch 44/160, Validation Loss: 0.000556\n",
      "Epoch 45/160, Validation Loss: 0.000446\n",
      "Epoch 46/160, Validation Loss: 0.000262\n",
      "Epoch 47/160, Validation Loss: 0.000325\n",
      "Epoch 48/160, Validation Loss: 0.000897\n",
      "Epoch 49/160, Validation Loss: 0.000183\n",
      "Epoch 50/160, Validation Loss: 0.001387\n",
      "Epoch 51/160, Validation Loss: 0.000188\n",
      "Epoch 52/160, Validation Loss: 0.000315\n",
      "Epoch 53/160, Validation Loss: 0.000454\n",
      "Epoch 54/160, Validation Loss: 0.000244\n",
      "Epoch 55/160, Validation Loss: 0.000533\n",
      "Epoch 56/160, Validation Loss: 0.000380\n",
      "Epoch 57/160, Validation Loss: 0.000198\n",
      "Epoch 58/160, Validation Loss: 0.000379\n",
      "Epoch 59/160, Validation Loss: 0.000335\n",
      "Epoch 60/160, Validation Loss: 0.000208\n",
      "Epoch 61/160, Validation Loss: 0.000283\n",
      "Epoch 62/160, Validation Loss: 0.001976\n",
      "Epoch 63/160, Validation Loss: 0.000234\n",
      "Epoch 64/160, Validation Loss: 0.000502\n",
      "Epoch 65/160, Validation Loss: 0.000249\n",
      "Epoch 66/160, Validation Loss: 0.000250\n",
      "Epoch 67/160, Validation Loss: 0.000273\n",
      "Epoch 68/160, Validation Loss: 0.000183\n",
      "Epoch 69/160, Validation Loss: 0.000184\n",
      "Epoch 70/160, Validation Loss: 0.000172\n",
      "Epoch 71/160, Validation Loss: 0.000258\n",
      "Epoch 72/160, Validation Loss: 0.000180\n",
      "Epoch 73/160, Validation Loss: 0.000234\n",
      "Epoch 74/160, Validation Loss: 0.000332\n",
      "Epoch 75/160, Validation Loss: 0.000234\n",
      "Epoch 76/160, Validation Loss: 0.000227\n",
      "Epoch 77/160, Validation Loss: 0.000774\n",
      "Epoch 78/160, Validation Loss: 0.000174\n",
      "Epoch 79/160, Validation Loss: 0.000180\n",
      "Epoch 80/160, Validation Loss: 0.000311\n",
      "Epoch 81/160, Validation Loss: 0.001075\n",
      "Epoch 82/160, Validation Loss: 0.000300\n",
      "Epoch 83/160, Validation Loss: 0.000631\n",
      "Epoch 84/160, Validation Loss: 0.000212\n",
      "Epoch 85/160, Validation Loss: 0.000195\n",
      "Epoch 86/160, Validation Loss: 0.000760\n",
      "Epoch 87/160, Validation Loss: 0.000170\n",
      "Epoch 88/160, Validation Loss: 0.000984\n",
      "Epoch 89/160, Validation Loss: 0.000178\n",
      "Epoch 90/160, Validation Loss: 0.000168\n",
      "Epoch 91/160, Validation Loss: 0.000385\n",
      "Epoch 92/160, Validation Loss: 0.000545\n",
      "Epoch 93/160, Validation Loss: 0.001203\n",
      "Epoch 94/160, Validation Loss: 0.000202\n",
      "Epoch 95/160, Validation Loss: 0.000194\n",
      "Epoch 96/160, Validation Loss: 0.000191\n",
      "Epoch 97/160, Validation Loss: 0.001404\n",
      "Epoch 98/160, Validation Loss: 0.000200\n",
      "Epoch 99/160, Validation Loss: 0.000353\n",
      "Epoch 100/160, Validation Loss: 0.000164\n",
      "Epoch 101/160, Validation Loss: 0.000384\n",
      "Epoch 102/160, Validation Loss: 0.000216\n",
      "Epoch 103/160, Validation Loss: 0.001372\n",
      "Epoch 104/160, Validation Loss: 0.000241\n",
      "Epoch 105/160, Validation Loss: 0.000201\n",
      "Epoch 106/160, Validation Loss: 0.000195\n",
      "Epoch 107/160, Validation Loss: 0.000180\n",
      "Epoch 108/160, Validation Loss: 0.000236\n",
      "Epoch 109/160, Validation Loss: 0.000258\n",
      "Epoch 110/160, Validation Loss: 0.000440\n",
      "Epoch 111/160, Validation Loss: 0.000246\n",
      "Epoch 112/160, Validation Loss: 0.000249\n",
      "Epoch 113/160, Validation Loss: 0.000495\n",
      "Epoch 114/160, Validation Loss: 0.000172\n",
      "Epoch 115/160, Validation Loss: 0.000489\n",
      "Epoch 116/160, Validation Loss: 0.000308\n",
      "Epoch 117/160, Validation Loss: 0.000201\n",
      "Epoch 118/160, Validation Loss: 0.000172\n",
      "Epoch 119/160, Validation Loss: 0.000359\n",
      "Epoch 120/160, Validation Loss: 0.000232\n",
      "Epoch 121/160, Validation Loss: 0.000596\n",
      "Epoch 122/160, Validation Loss: 0.000187\n",
      "Epoch 123/160, Validation Loss: 0.000259\n",
      "Epoch 124/160, Validation Loss: 0.000351\n",
      "Epoch 125/160, Validation Loss: 0.000327\n",
      "Epoch 126/160, Validation Loss: 0.000169\n",
      "Epoch 127/160, Validation Loss: 0.000367\n",
      "Epoch 128/160, Validation Loss: 0.000200\n",
      "Epoch 129/160, Validation Loss: 0.000274\n",
      "Epoch 130/160, Validation Loss: 0.000205\n",
      "Epoch 131/160, Validation Loss: 0.000191\n",
      "Epoch 132/160, Validation Loss: 0.000201\n",
      "Epoch 133/160, Validation Loss: 0.000358\n",
      "Epoch 134/160, Validation Loss: 0.000411\n",
      "Epoch 135/160, Validation Loss: 0.000263\n",
      "Epoch 136/160, Validation Loss: 0.000245\n",
      "Epoch 137/160, Validation Loss: 0.001290\n",
      "Epoch 138/160, Validation Loss: 0.000329\n",
      "Epoch 139/160, Validation Loss: 0.000191\n",
      "Epoch 140/160, Validation Loss: 0.000248\n",
      "Epoch 141/160, Validation Loss: 0.000556\n",
      "Epoch 142/160, Validation Loss: 0.000170\n",
      "Epoch 143/160, Validation Loss: 0.000189\n",
      "Epoch 144/160, Validation Loss: 0.001341\n",
      "Epoch 145/160, Validation Loss: 0.000167\n",
      "Epoch 146/160, Validation Loss: 0.000178\n",
      "Epoch 147/160, Validation Loss: 0.000166\n",
      "Epoch 148/160, Validation Loss: 0.000288\n",
      "Epoch 149/160, Validation Loss: 0.000232\n",
      "Epoch 150/160, Validation Loss: 0.000179\n",
      "Epoch 151/160, Validation Loss: 0.000598\n",
      "Epoch 152/160, Validation Loss: 0.000329\n",
      "Epoch 153/160, Validation Loss: 0.000263\n",
      "Epoch 154/160, Validation Loss: 0.000191\n",
      "Epoch 155/160, Validation Loss: 0.000172\n",
      "Epoch 156/160, Validation Loss: 0.000224\n",
      "Epoch 157/160, Validation Loss: 0.000385\n",
      "Epoch 158/160, Validation Loss: 0.000297\n",
      "Epoch 159/160, Validation Loss: 0.000199\n",
      "Epoch 160/160, Validation Loss: 0.000303\n",
      "Training model with 7 layers and 70 neurons per layer...\n",
      "Epoch 1/160, Validation Loss: 0.001178\n",
      "Epoch 2/160, Validation Loss: 0.000731\n",
      "Epoch 3/160, Validation Loss: 0.000651\n",
      "Epoch 4/160, Validation Loss: 0.000506\n",
      "Epoch 5/160, Validation Loss: 0.000495\n",
      "Epoch 6/160, Validation Loss: 0.000501\n",
      "Epoch 7/160, Validation Loss: 0.000403\n",
      "Epoch 8/160, Validation Loss: 0.000376\n",
      "Epoch 9/160, Validation Loss: 0.000344\n",
      "Epoch 10/160, Validation Loss: 0.000547\n",
      "Epoch 11/160, Validation Loss: 0.000625\n",
      "Epoch 12/160, Validation Loss: 0.000447\n",
      "Epoch 13/160, Validation Loss: 0.000389\n",
      "Epoch 14/160, Validation Loss: 0.000511\n",
      "Epoch 15/160, Validation Loss: 0.000417\n",
      "Epoch 16/160, Validation Loss: 0.000285\n",
      "Epoch 17/160, Validation Loss: 0.000369\n",
      "Epoch 18/160, Validation Loss: 0.000423\n",
      "Epoch 19/160, Validation Loss: 0.000322\n",
      "Epoch 20/160, Validation Loss: 0.000255\n",
      "Epoch 21/160, Validation Loss: 0.001228\n",
      "Epoch 22/160, Validation Loss: 0.000535\n",
      "Epoch 23/160, Validation Loss: 0.000228\n",
      "Epoch 24/160, Validation Loss: 0.000336\n",
      "Epoch 25/160, Validation Loss: 0.000343\n",
      "Epoch 26/160, Validation Loss: 0.000548\n",
      "Epoch 27/160, Validation Loss: 0.001162\n",
      "Epoch 28/160, Validation Loss: 0.000220\n",
      "Epoch 29/160, Validation Loss: 0.000379\n",
      "Epoch 30/160, Validation Loss: 0.000365\n",
      "Epoch 31/160, Validation Loss: 0.000616\n",
      "Epoch 32/160, Validation Loss: 0.000610\n",
      "Epoch 33/160, Validation Loss: 0.000433\n",
      "Epoch 34/160, Validation Loss: 0.000261\n",
      "Epoch 35/160, Validation Loss: 0.000856\n",
      "Epoch 36/160, Validation Loss: 0.000219\n",
      "Epoch 37/160, Validation Loss: 0.000226\n",
      "Epoch 38/160, Validation Loss: 0.000321\n",
      "Epoch 39/160, Validation Loss: 0.000194\n",
      "Epoch 40/160, Validation Loss: 0.000267\n",
      "Epoch 41/160, Validation Loss: 0.000207\n",
      "Epoch 42/160, Validation Loss: 0.000236\n",
      "Epoch 43/160, Validation Loss: 0.001766\n",
      "Epoch 44/160, Validation Loss: 0.000244\n",
      "Epoch 45/160, Validation Loss: 0.000451\n",
      "Epoch 46/160, Validation Loss: 0.000396\n",
      "Epoch 47/160, Validation Loss: 0.000247\n",
      "Epoch 48/160, Validation Loss: 0.000223\n",
      "Epoch 49/160, Validation Loss: 0.000445\n",
      "Epoch 50/160, Validation Loss: 0.000190\n",
      "Epoch 51/160, Validation Loss: 0.000573\n",
      "Epoch 52/160, Validation Loss: 0.000448\n",
      "Epoch 53/160, Validation Loss: 0.000264\n",
      "Epoch 54/160, Validation Loss: 0.000476\n",
      "Epoch 55/160, Validation Loss: 0.000276\n",
      "Epoch 56/160, Validation Loss: 0.000594\n",
      "Epoch 57/160, Validation Loss: 0.000205\n",
      "Epoch 58/160, Validation Loss: 0.000207\n",
      "Epoch 59/160, Validation Loss: 0.000181\n",
      "Epoch 60/160, Validation Loss: 0.000280\n",
      "Epoch 61/160, Validation Loss: 0.000200\n",
      "Epoch 62/160, Validation Loss: 0.002042\n",
      "Epoch 63/160, Validation Loss: 0.000220\n",
      "Epoch 64/160, Validation Loss: 0.000412\n",
      "Epoch 65/160, Validation Loss: 0.000172\n",
      "Epoch 66/160, Validation Loss: 0.000345\n",
      "Epoch 67/160, Validation Loss: 0.000396\n",
      "Epoch 68/160, Validation Loss: 0.000373\n",
      "Epoch 69/160, Validation Loss: 0.000318\n",
      "Epoch 70/160, Validation Loss: 0.000764\n",
      "Epoch 71/160, Validation Loss: 0.000580\n",
      "Epoch 72/160, Validation Loss: 0.000185\n",
      "Epoch 73/160, Validation Loss: 0.000183\n",
      "Epoch 74/160, Validation Loss: 0.000174\n",
      "Epoch 75/160, Validation Loss: 0.000164\n",
      "Epoch 76/160, Validation Loss: 0.000236\n",
      "Epoch 77/160, Validation Loss: 0.000266\n",
      "Epoch 78/160, Validation Loss: 0.000171\n",
      "Epoch 79/160, Validation Loss: 0.000205\n",
      "Epoch 80/160, Validation Loss: 0.001547\n",
      "Epoch 81/160, Validation Loss: 0.000186\n",
      "Epoch 82/160, Validation Loss: 0.000705\n",
      "Epoch 83/160, Validation Loss: 0.000235\n",
      "Epoch 84/160, Validation Loss: 0.000364\n",
      "Epoch 85/160, Validation Loss: 0.000300\n",
      "Epoch 86/160, Validation Loss: 0.000566\n",
      "Epoch 87/160, Validation Loss: 0.000183\n",
      "Epoch 88/160, Validation Loss: 0.000184\n",
      "Epoch 89/160, Validation Loss: 0.000206\n",
      "Epoch 90/160, Validation Loss: 0.000215\n",
      "Epoch 91/160, Validation Loss: 0.000179\n",
      "Epoch 92/160, Validation Loss: 0.000261\n",
      "Epoch 93/160, Validation Loss: 0.000162\n",
      "Epoch 94/160, Validation Loss: 0.000475\n",
      "Epoch 95/160, Validation Loss: 0.000362\n",
      "Epoch 96/160, Validation Loss: 0.001397\n",
      "Epoch 97/160, Validation Loss: 0.001087\n",
      "Epoch 98/160, Validation Loss: 0.000206\n",
      "Epoch 99/160, Validation Loss: 0.000163\n",
      "Epoch 100/160, Validation Loss: 0.000162\n",
      "Epoch 101/160, Validation Loss: 0.000213\n",
      "Epoch 102/160, Validation Loss: 0.001254\n",
      "Epoch 103/160, Validation Loss: 0.000161\n",
      "Epoch 104/160, Validation Loss: 0.000198\n",
      "Epoch 105/160, Validation Loss: 0.000229\n",
      "Epoch 106/160, Validation Loss: 0.000985\n",
      "Epoch 107/160, Validation Loss: 0.000385\n",
      "Epoch 108/160, Validation Loss: 0.000165\n",
      "Epoch 109/160, Validation Loss: 0.000303\n",
      "Epoch 110/160, Validation Loss: 0.000173\n",
      "Epoch 111/160, Validation Loss: 0.000339\n",
      "Epoch 112/160, Validation Loss: 0.000183\n",
      "Epoch 113/160, Validation Loss: 0.000569\n",
      "Epoch 114/160, Validation Loss: 0.000224\n",
      "Epoch 115/160, Validation Loss: 0.000255\n",
      "Epoch 116/160, Validation Loss: 0.000222\n",
      "Epoch 117/160, Validation Loss: 0.000173\n",
      "Epoch 118/160, Validation Loss: 0.000214\n",
      "Epoch 119/160, Validation Loss: 0.001258\n",
      "Epoch 120/160, Validation Loss: 0.000313\n",
      "Epoch 121/160, Validation Loss: 0.000197\n",
      "Epoch 122/160, Validation Loss: 0.000218\n",
      "Epoch 123/160, Validation Loss: 0.000199\n",
      "Epoch 124/160, Validation Loss: 0.000183\n",
      "Epoch 125/160, Validation Loss: 0.000180\n",
      "Epoch 126/160, Validation Loss: 0.000217\n",
      "Epoch 127/160, Validation Loss: 0.000257\n",
      "Epoch 128/160, Validation Loss: 0.000322\n",
      "Epoch 129/160, Validation Loss: 0.000249\n",
      "Epoch 130/160, Validation Loss: 0.000212\n",
      "Epoch 131/160, Validation Loss: 0.000264\n",
      "Epoch 132/160, Validation Loss: 0.001485\n",
      "Epoch 133/160, Validation Loss: 0.000190\n",
      "Epoch 134/160, Validation Loss: 0.000172\n",
      "Epoch 135/160, Validation Loss: 0.000227\n",
      "Epoch 136/160, Validation Loss: 0.000212\n",
      "Epoch 137/160, Validation Loss: 0.000585\n",
      "Epoch 138/160, Validation Loss: 0.000358\n",
      "Epoch 139/160, Validation Loss: 0.000322\n",
      "Epoch 140/160, Validation Loss: 0.000178\n",
      "Epoch 141/160, Validation Loss: 0.000425\n",
      "Epoch 142/160, Validation Loss: 0.000173\n",
      "Epoch 143/160, Validation Loss: 0.000330\n",
      "Epoch 144/160, Validation Loss: 0.000178\n",
      "Epoch 145/160, Validation Loss: 0.001575\n",
      "Epoch 146/160, Validation Loss: 0.000168\n",
      "Epoch 147/160, Validation Loss: 0.000325\n",
      "Epoch 148/160, Validation Loss: 0.000217\n",
      "Epoch 149/160, Validation Loss: 0.000368\n",
      "Epoch 150/160, Validation Loss: 0.000265\n",
      "Epoch 151/160, Validation Loss: 0.002196\n",
      "Epoch 152/160, Validation Loss: 0.000175\n",
      "Epoch 153/160, Validation Loss: 0.000482\n",
      "Epoch 154/160, Validation Loss: 0.000167\n",
      "Epoch 155/160, Validation Loss: 0.000260\n",
      "Epoch 156/160, Validation Loss: 0.000341\n",
      "Epoch 157/160, Validation Loss: 0.000520\n",
      "Epoch 158/160, Validation Loss: 0.000221\n",
      "Epoch 159/160, Validation Loss: 0.001349\n",
      "Epoch 160/160, Validation Loss: 0.000226\n",
      "Training model with 8 layers and 50 neurons per layer...\n",
      "Epoch 1/160, Validation Loss: 0.001306\n",
      "Epoch 2/160, Validation Loss: 0.001244\n",
      "Epoch 3/160, Validation Loss: 0.000594\n",
      "Epoch 4/160, Validation Loss: 0.001056\n",
      "Epoch 5/160, Validation Loss: 0.000438\n",
      "Epoch 6/160, Validation Loss: 0.000478\n",
      "Epoch 7/160, Validation Loss: 0.000388\n",
      "Epoch 8/160, Validation Loss: 0.000446\n",
      "Epoch 9/160, Validation Loss: 0.000727\n",
      "Epoch 10/160, Validation Loss: 0.000382\n",
      "Epoch 11/160, Validation Loss: 0.000396\n",
      "Epoch 12/160, Validation Loss: 0.001383\n",
      "Epoch 13/160, Validation Loss: 0.000549\n",
      "Epoch 14/160, Validation Loss: 0.000298\n",
      "Epoch 15/160, Validation Loss: 0.000372\n",
      "Epoch 16/160, Validation Loss: 0.000650\n",
      "Epoch 17/160, Validation Loss: 0.000298\n",
      "Epoch 18/160, Validation Loss: 0.001194\n",
      "Epoch 19/160, Validation Loss: 0.000434\n",
      "Epoch 20/160, Validation Loss: 0.000265\n",
      "Epoch 21/160, Validation Loss: 0.000257\n",
      "Epoch 22/160, Validation Loss: 0.000293\n",
      "Epoch 23/160, Validation Loss: 0.000744\n",
      "Epoch 24/160, Validation Loss: 0.000275\n",
      "Epoch 25/160, Validation Loss: 0.000264\n",
      "Epoch 26/160, Validation Loss: 0.000295\n",
      "Epoch 27/160, Validation Loss: 0.000474\n",
      "Epoch 28/160, Validation Loss: 0.000369\n",
      "Epoch 29/160, Validation Loss: 0.000310\n",
      "Epoch 30/160, Validation Loss: 0.000354\n",
      "Epoch 31/160, Validation Loss: 0.000557\n",
      "Epoch 32/160, Validation Loss: 0.000441\n",
      "Epoch 33/160, Validation Loss: 0.000311\n",
      "Epoch 34/160, Validation Loss: 0.000208\n",
      "Epoch 35/160, Validation Loss: 0.000302\n",
      "Epoch 36/160, Validation Loss: 0.000354\n",
      "Epoch 37/160, Validation Loss: 0.000201\n",
      "Epoch 38/160, Validation Loss: 0.000541\n",
      "Epoch 39/160, Validation Loss: 0.001225\n",
      "Epoch 40/160, Validation Loss: 0.000205\n",
      "Epoch 41/160, Validation Loss: 0.000274\n",
      "Epoch 42/160, Validation Loss: 0.000209\n",
      "Epoch 43/160, Validation Loss: 0.000238\n",
      "Epoch 44/160, Validation Loss: 0.000236\n",
      "Epoch 45/160, Validation Loss: 0.000496\n",
      "Epoch 46/160, Validation Loss: 0.000214\n",
      "Epoch 47/160, Validation Loss: 0.000194\n",
      "Epoch 48/160, Validation Loss: 0.000336\n",
      "Epoch 49/160, Validation Loss: 0.001632\n",
      "Epoch 50/160, Validation Loss: 0.001995\n",
      "Epoch 51/160, Validation Loss: 0.001517\n",
      "Epoch 52/160, Validation Loss: 0.000202\n",
      "Epoch 53/160, Validation Loss: 0.000264\n",
      "Epoch 54/160, Validation Loss: 0.000271\n",
      "Epoch 55/160, Validation Loss: 0.000374\n",
      "Epoch 56/160, Validation Loss: 0.000266\n",
      "Epoch 57/160, Validation Loss: 0.002000\n",
      "Epoch 58/160, Validation Loss: 0.000504\n",
      "Epoch 59/160, Validation Loss: 0.000207\n",
      "Epoch 60/160, Validation Loss: 0.000237\n",
      "Epoch 61/160, Validation Loss: 0.000553\n",
      "Epoch 62/160, Validation Loss: 0.000177\n",
      "Epoch 63/160, Validation Loss: 0.000312\n",
      "Epoch 64/160, Validation Loss: 0.000855\n",
      "Epoch 65/160, Validation Loss: 0.000235\n",
      "Epoch 66/160, Validation Loss: 0.000177\n",
      "Epoch 67/160, Validation Loss: 0.000365\n",
      "Epoch 68/160, Validation Loss: 0.000254\n",
      "Epoch 69/160, Validation Loss: 0.000638\n",
      "Epoch 70/160, Validation Loss: 0.000307\n",
      "Epoch 71/160, Validation Loss: 0.000274\n",
      "Epoch 72/160, Validation Loss: 0.000574\n",
      "Epoch 73/160, Validation Loss: 0.000540\n",
      "Epoch 74/160, Validation Loss: 0.000306\n",
      "Epoch 75/160, Validation Loss: 0.001190\n",
      "Epoch 76/160, Validation Loss: 0.000243\n",
      "Epoch 77/160, Validation Loss: 0.000415\n",
      "Epoch 78/160, Validation Loss: 0.000194\n",
      "Epoch 79/160, Validation Loss: 0.000544\n",
      "Epoch 80/160, Validation Loss: 0.000187\n",
      "Epoch 81/160, Validation Loss: 0.000203\n",
      "Epoch 82/160, Validation Loss: 0.000170\n",
      "Epoch 83/160, Validation Loss: 0.000238\n",
      "Epoch 84/160, Validation Loss: 0.000375\n",
      "Epoch 85/160, Validation Loss: 0.000217\n",
      "Epoch 86/160, Validation Loss: 0.001147\n",
      "Epoch 87/160, Validation Loss: 0.000261\n",
      "Epoch 88/160, Validation Loss: 0.000240\n",
      "Epoch 89/160, Validation Loss: 0.000336\n",
      "Epoch 90/160, Validation Loss: 0.000402\n",
      "Epoch 91/160, Validation Loss: 0.000202\n",
      "Epoch 92/160, Validation Loss: 0.000888\n",
      "Epoch 93/160, Validation Loss: 0.000184\n",
      "Epoch 94/160, Validation Loss: 0.000197\n",
      "Epoch 95/160, Validation Loss: 0.000888\n",
      "Epoch 96/160, Validation Loss: 0.000246\n",
      "Epoch 97/160, Validation Loss: 0.002193\n",
      "Epoch 98/160, Validation Loss: 0.000272\n",
      "Epoch 99/160, Validation Loss: 0.000172\n",
      "Epoch 100/160, Validation Loss: 0.000170\n",
      "Epoch 101/160, Validation Loss: 0.000667\n",
      "Epoch 102/160, Validation Loss: 0.000177\n",
      "Epoch 103/160, Validation Loss: 0.000322\n",
      "Epoch 104/160, Validation Loss: 0.000237\n",
      "Epoch 105/160, Validation Loss: 0.000256\n",
      "Epoch 106/160, Validation Loss: 0.000204\n",
      "Epoch 107/160, Validation Loss: 0.000382\n",
      "Epoch 108/160, Validation Loss: 0.000209\n",
      "Epoch 109/160, Validation Loss: 0.000512\n",
      "Epoch 110/160, Validation Loss: 0.000175\n",
      "Epoch 111/160, Validation Loss: 0.000189\n",
      "Epoch 112/160, Validation Loss: 0.000238\n",
      "Epoch 113/160, Validation Loss: 0.000709\n",
      "Epoch 114/160, Validation Loss: 0.000644\n",
      "Epoch 115/160, Validation Loss: 0.000158\n",
      "Epoch 116/160, Validation Loss: 0.000176\n",
      "Epoch 117/160, Validation Loss: 0.000302\n",
      "Epoch 118/160, Validation Loss: 0.000347\n",
      "Epoch 119/160, Validation Loss: 0.000234\n",
      "Epoch 120/160, Validation Loss: 0.000215\n",
      "Epoch 121/160, Validation Loss: 0.000782\n",
      "Epoch 122/160, Validation Loss: 0.000260\n",
      "Epoch 123/160, Validation Loss: 0.000170\n",
      "Epoch 124/160, Validation Loss: 0.000218\n",
      "Epoch 125/160, Validation Loss: 0.000208\n",
      "Epoch 126/160, Validation Loss: 0.000179\n",
      "Epoch 127/160, Validation Loss: 0.000170\n",
      "Epoch 128/160, Validation Loss: 0.000741\n",
      "Epoch 129/160, Validation Loss: 0.000393\n",
      "Epoch 130/160, Validation Loss: 0.000303\n",
      "Epoch 131/160, Validation Loss: 0.000183\n",
      "Epoch 132/160, Validation Loss: 0.000941\n",
      "Epoch 133/160, Validation Loss: 0.000241\n",
      "Epoch 134/160, Validation Loss: 0.000450\n",
      "Epoch 135/160, Validation Loss: 0.000303\n",
      "Epoch 136/160, Validation Loss: 0.000232\n",
      "Epoch 137/160, Validation Loss: 0.000333\n",
      "Epoch 138/160, Validation Loss: 0.000166\n",
      "Epoch 139/160, Validation Loss: 0.000163\n",
      "Epoch 140/160, Validation Loss: 0.000172\n",
      "Epoch 141/160, Validation Loss: 0.000307\n",
      "Epoch 142/160, Validation Loss: 0.000389\n",
      "Epoch 143/160, Validation Loss: 0.000220\n",
      "Epoch 144/160, Validation Loss: 0.000248\n",
      "Epoch 145/160, Validation Loss: 0.000319\n",
      "Epoch 146/160, Validation Loss: 0.000351\n",
      "Epoch 147/160, Validation Loss: 0.000186\n",
      "Epoch 148/160, Validation Loss: 0.000167\n",
      "Epoch 149/160, Validation Loss: 0.000200\n",
      "Epoch 150/160, Validation Loss: 0.000704\n",
      "Epoch 151/160, Validation Loss: 0.000234\n",
      "Epoch 152/160, Validation Loss: 0.000308\n",
      "Epoch 153/160, Validation Loss: 0.000252\n",
      "Epoch 154/160, Validation Loss: 0.000254\n",
      "Epoch 155/160, Validation Loss: 0.000247\n",
      "Epoch 156/160, Validation Loss: 0.000931\n",
      "Epoch 157/160, Validation Loss: 0.000298\n",
      "Epoch 158/160, Validation Loss: 0.000187\n",
      "Epoch 159/160, Validation Loss: 0.000193\n",
      "Epoch 160/160, Validation Loss: 0.000221\n",
      "Training model with 8 layers and 60 neurons per layer...\n",
      "Epoch 1/160, Validation Loss: 0.001305\n",
      "Epoch 2/160, Validation Loss: 0.000657\n",
      "Epoch 3/160, Validation Loss: 0.000557\n",
      "Epoch 4/160, Validation Loss: 0.000550\n",
      "Epoch 5/160, Validation Loss: 0.000424\n",
      "Epoch 6/160, Validation Loss: 0.000527\n",
      "Epoch 7/160, Validation Loss: 0.000417\n",
      "Epoch 8/160, Validation Loss: 0.000354\n",
      "Epoch 9/160, Validation Loss: 0.000446\n",
      "Epoch 10/160, Validation Loss: 0.000761\n",
      "Epoch 11/160, Validation Loss: 0.000400\n",
      "Epoch 12/160, Validation Loss: 0.000387\n",
      "Epoch 13/160, Validation Loss: 0.000337\n",
      "Epoch 14/160, Validation Loss: 0.000431\n",
      "Epoch 15/160, Validation Loss: 0.000481\n",
      "Epoch 16/160, Validation Loss: 0.000262\n",
      "Epoch 17/160, Validation Loss: 0.000320\n",
      "Epoch 18/160, Validation Loss: 0.000292\n",
      "Epoch 19/160, Validation Loss: 0.000434\n",
      "Epoch 20/160, Validation Loss: 0.000381\n",
      "Epoch 21/160, Validation Loss: 0.000709\n",
      "Epoch 22/160, Validation Loss: 0.000319\n",
      "Epoch 23/160, Validation Loss: 0.000487\n",
      "Epoch 24/160, Validation Loss: 0.000257\n",
      "Epoch 25/160, Validation Loss: 0.000310\n",
      "Epoch 26/160, Validation Loss: 0.000225\n",
      "Epoch 27/160, Validation Loss: 0.000759\n",
      "Epoch 28/160, Validation Loss: 0.000224\n",
      "Epoch 29/160, Validation Loss: 0.000272\n",
      "Epoch 30/160, Validation Loss: 0.000230\n",
      "Epoch 31/160, Validation Loss: 0.000471\n",
      "Epoch 32/160, Validation Loss: 0.000345\n",
      "Epoch 33/160, Validation Loss: 0.000234\n",
      "Epoch 34/160, Validation Loss: 0.000225\n",
      "Epoch 35/160, Validation Loss: 0.000676\n",
      "Epoch 36/160, Validation Loss: 0.001057\n",
      "Epoch 37/160, Validation Loss: 0.000403\n",
      "Epoch 38/160, Validation Loss: 0.000427\n",
      "Epoch 39/160, Validation Loss: 0.000221\n",
      "Epoch 40/160, Validation Loss: 0.000287\n",
      "Epoch 41/160, Validation Loss: 0.000225\n",
      "Epoch 42/160, Validation Loss: 0.000345\n",
      "Epoch 43/160, Validation Loss: 0.000553\n",
      "Epoch 44/160, Validation Loss: 0.001402\n",
      "Epoch 45/160, Validation Loss: 0.000613\n",
      "Epoch 46/160, Validation Loss: 0.000226\n",
      "Epoch 47/160, Validation Loss: 0.000332\n",
      "Epoch 48/160, Validation Loss: 0.000294\n",
      "Epoch 49/160, Validation Loss: 0.000181\n",
      "Epoch 50/160, Validation Loss: 0.000368\n",
      "Epoch 51/160, Validation Loss: 0.000282\n",
      "Epoch 52/160, Validation Loss: 0.000488\n",
      "Epoch 53/160, Validation Loss: 0.000179\n",
      "Epoch 54/160, Validation Loss: 0.000204\n",
      "Epoch 55/160, Validation Loss: 0.001178\n",
      "Epoch 56/160, Validation Loss: 0.000916\n",
      "Epoch 57/160, Validation Loss: 0.000507\n",
      "Epoch 58/160, Validation Loss: 0.000241\n",
      "Epoch 59/160, Validation Loss: 0.001044\n",
      "Epoch 60/160, Validation Loss: 0.000286\n",
      "Epoch 61/160, Validation Loss: 0.000187\n",
      "Epoch 62/160, Validation Loss: 0.000274\n",
      "Epoch 63/160, Validation Loss: 0.000423\n",
      "Epoch 64/160, Validation Loss: 0.000199\n",
      "Epoch 65/160, Validation Loss: 0.000282\n",
      "Epoch 66/160, Validation Loss: 0.000197\n",
      "Epoch 67/160, Validation Loss: 0.000598\n",
      "Epoch 68/160, Validation Loss: 0.000589\n",
      "Epoch 69/160, Validation Loss: 0.000594\n",
      "Epoch 70/160, Validation Loss: 0.001542\n",
      "Epoch 71/160, Validation Loss: 0.000207\n",
      "Epoch 72/160, Validation Loss: 0.000247\n",
      "Epoch 73/160, Validation Loss: 0.000734\n",
      "Epoch 74/160, Validation Loss: 0.000388\n",
      "Epoch 75/160, Validation Loss: 0.000181\n",
      "Epoch 76/160, Validation Loss: 0.000186\n",
      "Epoch 77/160, Validation Loss: 0.000286\n",
      "Epoch 78/160, Validation Loss: 0.000344\n",
      "Epoch 79/160, Validation Loss: 0.000187\n",
      "Epoch 80/160, Validation Loss: 0.001194\n",
      "Epoch 81/160, Validation Loss: 0.000232\n",
      "Epoch 82/160, Validation Loss: 0.001032\n",
      "Epoch 83/160, Validation Loss: 0.000525\n",
      "Epoch 84/160, Validation Loss: 0.000173\n",
      "Epoch 85/160, Validation Loss: 0.000470\n",
      "Epoch 86/160, Validation Loss: 0.000309\n",
      "Epoch 87/160, Validation Loss: 0.000172\n",
      "Epoch 88/160, Validation Loss: 0.000180\n",
      "Epoch 89/160, Validation Loss: 0.000287\n",
      "Epoch 90/160, Validation Loss: 0.000484\n",
      "Epoch 91/160, Validation Loss: 0.000166\n",
      "Epoch 92/160, Validation Loss: 0.000210\n",
      "Epoch 93/160, Validation Loss: 0.000442\n",
      "Epoch 94/160, Validation Loss: 0.000184\n",
      "Epoch 95/160, Validation Loss: 0.000717\n",
      "Epoch 96/160, Validation Loss: 0.000200\n",
      "Epoch 97/160, Validation Loss: 0.000392\n",
      "Epoch 98/160, Validation Loss: 0.001859\n",
      "Epoch 99/160, Validation Loss: 0.000313\n",
      "Epoch 100/160, Validation Loss: 0.000584\n",
      "Epoch 101/160, Validation Loss: 0.000634\n",
      "Epoch 102/160, Validation Loss: 0.000346\n",
      "Epoch 103/160, Validation Loss: 0.000204\n",
      "Epoch 104/160, Validation Loss: 0.000233\n",
      "Epoch 105/160, Validation Loss: 0.000340\n",
      "Epoch 106/160, Validation Loss: 0.000179\n",
      "Epoch 107/160, Validation Loss: 0.000396\n",
      "Epoch 108/160, Validation Loss: 0.000198\n",
      "Epoch 109/160, Validation Loss: 0.000397\n",
      "Epoch 110/160, Validation Loss: 0.000165\n",
      "Epoch 111/160, Validation Loss: 0.000226\n",
      "Epoch 112/160, Validation Loss: 0.000285\n",
      "Epoch 113/160, Validation Loss: 0.000289\n",
      "Epoch 114/160, Validation Loss: 0.000207\n",
      "Epoch 115/160, Validation Loss: 0.000166\n",
      "Epoch 116/160, Validation Loss: 0.000191\n",
      "Epoch 117/160, Validation Loss: 0.000183\n",
      "Epoch 118/160, Validation Loss: 0.000216\n",
      "Epoch 119/160, Validation Loss: 0.000175\n",
      "Epoch 120/160, Validation Loss: 0.000200\n",
      "Epoch 121/160, Validation Loss: 0.000666\n",
      "Epoch 122/160, Validation Loss: 0.000251\n",
      "Epoch 123/160, Validation Loss: 0.000175\n",
      "Epoch 124/160, Validation Loss: 0.000171\n",
      "Epoch 125/160, Validation Loss: 0.000632\n",
      "Epoch 126/160, Validation Loss: 0.000210\n",
      "Epoch 127/160, Validation Loss: 0.000206\n",
      "Epoch 128/160, Validation Loss: 0.000302\n",
      "Epoch 129/160, Validation Loss: 0.000444\n",
      "Epoch 130/160, Validation Loss: 0.000713\n",
      "Epoch 131/160, Validation Loss: 0.001122\n",
      "Epoch 132/160, Validation Loss: 0.000237\n",
      "Epoch 133/160, Validation Loss: 0.000188\n",
      "Epoch 134/160, Validation Loss: 0.000162\n",
      "Epoch 135/160, Validation Loss: 0.000756\n",
      "Epoch 136/160, Validation Loss: 0.000216\n",
      "Epoch 137/160, Validation Loss: 0.000184\n",
      "Epoch 138/160, Validation Loss: 0.000314\n",
      "Epoch 139/160, Validation Loss: 0.000181\n",
      "Epoch 140/160, Validation Loss: 0.000164\n",
      "Epoch 141/160, Validation Loss: 0.000274\n",
      "Epoch 142/160, Validation Loss: 0.000446\n",
      "Epoch 143/160, Validation Loss: 0.000187\n",
      "Epoch 144/160, Validation Loss: 0.000232\n",
      "Epoch 145/160, Validation Loss: 0.000168\n",
      "Epoch 146/160, Validation Loss: 0.000444\n",
      "Epoch 147/160, Validation Loss: 0.000165\n",
      "Epoch 148/160, Validation Loss: 0.000275\n",
      "Epoch 149/160, Validation Loss: 0.000231\n",
      "Epoch 150/160, Validation Loss: 0.000185\n",
      "Epoch 151/160, Validation Loss: 0.000409\n",
      "Epoch 152/160, Validation Loss: 0.000245\n",
      "Epoch 153/160, Validation Loss: 0.000293\n",
      "Epoch 154/160, Validation Loss: 0.000203\n",
      "Epoch 155/160, Validation Loss: 0.000442\n",
      "Epoch 156/160, Validation Loss: 0.000175\n",
      "Epoch 157/160, Validation Loss: 0.000217\n",
      "Epoch 158/160, Validation Loss: 0.000235\n",
      "Epoch 159/160, Validation Loss: 0.000445\n",
      "Epoch 160/160, Validation Loss: 0.000233\n",
      "Training model with 8 layers and 70 neurons per layer...\n",
      "Epoch 1/160, Validation Loss: 0.001301\n",
      "Epoch 2/160, Validation Loss: 0.000761\n",
      "Epoch 3/160, Validation Loss: 0.000509\n",
      "Epoch 4/160, Validation Loss: 0.000580\n",
      "Epoch 5/160, Validation Loss: 0.000371\n",
      "Epoch 6/160, Validation Loss: 0.000548\n",
      "Epoch 7/160, Validation Loss: 0.000395\n",
      "Epoch 8/160, Validation Loss: 0.000335\n",
      "Epoch 9/160, Validation Loss: 0.000293\n",
      "Epoch 10/160, Validation Loss: 0.000275\n",
      "Epoch 11/160, Validation Loss: 0.000317\n",
      "Epoch 12/160, Validation Loss: 0.000362\n",
      "Epoch 13/160, Validation Loss: 0.000339\n",
      "Epoch 14/160, Validation Loss: 0.000226\n",
      "Epoch 15/160, Validation Loss: 0.000227\n",
      "Epoch 16/160, Validation Loss: 0.000244\n",
      "Epoch 17/160, Validation Loss: 0.000754\n",
      "Epoch 18/160, Validation Loss: 0.000369\n",
      "Epoch 19/160, Validation Loss: 0.005073\n",
      "Epoch 20/160, Validation Loss: 0.000268\n",
      "Epoch 21/160, Validation Loss: 0.000461\n",
      "Epoch 22/160, Validation Loss: 0.000400\n",
      "Epoch 23/160, Validation Loss: 0.000387\n",
      "Epoch 24/160, Validation Loss: 0.000217\n",
      "Epoch 25/160, Validation Loss: 0.000203\n",
      "Epoch 26/160, Validation Loss: 0.000289\n",
      "Epoch 27/160, Validation Loss: 0.000199\n",
      "Epoch 28/160, Validation Loss: 0.000325\n",
      "Epoch 29/160, Validation Loss: 0.000396\n",
      "Epoch 30/160, Validation Loss: 0.000742\n",
      "Epoch 31/160, Validation Loss: 0.000232\n",
      "Epoch 32/160, Validation Loss: 0.000190\n",
      "Epoch 33/160, Validation Loss: 0.000482\n",
      "Epoch 34/160, Validation Loss: 0.000177\n",
      "Epoch 35/160, Validation Loss: 0.000593\n",
      "Epoch 36/160, Validation Loss: 0.000501\n",
      "Epoch 37/160, Validation Loss: 0.000238\n",
      "Epoch 38/160, Validation Loss: 0.000232\n",
      "Epoch 39/160, Validation Loss: 0.000995\n",
      "Epoch 40/160, Validation Loss: 0.000176\n",
      "Epoch 41/160, Validation Loss: 0.000370\n",
      "Epoch 42/160, Validation Loss: 0.000306\n",
      "Epoch 43/160, Validation Loss: 0.000262\n",
      "Epoch 44/160, Validation Loss: 0.000475\n",
      "Epoch 45/160, Validation Loss: 0.000172\n",
      "Epoch 46/160, Validation Loss: 0.000564\n",
      "Epoch 47/160, Validation Loss: 0.000180\n",
      "Epoch 48/160, Validation Loss: 0.000616\n",
      "Epoch 49/160, Validation Loss: 0.000849\n",
      "Epoch 50/160, Validation Loss: 0.000652\n",
      "Epoch 51/160, Validation Loss: 0.000169\n",
      "Epoch 52/160, Validation Loss: 0.000232\n",
      "Epoch 53/160, Validation Loss: 0.000633\n",
      "Epoch 54/160, Validation Loss: 0.000566\n",
      "Epoch 55/160, Validation Loss: 0.000228\n",
      "Epoch 56/160, Validation Loss: 0.000192\n",
      "Epoch 57/160, Validation Loss: 0.000414\n",
      "Epoch 58/160, Validation Loss: 0.000302\n",
      "Epoch 59/160, Validation Loss: 0.000231\n",
      "Epoch 60/160, Validation Loss: 0.000528\n",
      "Epoch 61/160, Validation Loss: 0.000333\n",
      "Epoch 62/160, Validation Loss: 0.000169\n",
      "Epoch 63/160, Validation Loss: 0.000249\n",
      "Epoch 64/160, Validation Loss: 0.000979\n",
      "Epoch 65/160, Validation Loss: 0.000875\n",
      "Epoch 66/160, Validation Loss: 0.000387\n",
      "Epoch 67/160, Validation Loss: 0.001908\n",
      "Epoch 68/160, Validation Loss: 0.000352\n",
      "Epoch 69/160, Validation Loss: 0.000426\n",
      "Epoch 70/160, Validation Loss: 0.000251\n",
      "Epoch 71/160, Validation Loss: 0.000190\n",
      "Epoch 72/160, Validation Loss: 0.000268\n",
      "Epoch 73/160, Validation Loss: 0.000507\n",
      "Epoch 74/160, Validation Loss: 0.000380\n",
      "Epoch 75/160, Validation Loss: 0.000202\n",
      "Epoch 76/160, Validation Loss: 0.000606\n",
      "Epoch 77/160, Validation Loss: 0.000352\n",
      "Epoch 78/160, Validation Loss: 0.000213\n",
      "Epoch 79/160, Validation Loss: 0.000173\n",
      "Epoch 80/160, Validation Loss: 0.000298\n",
      "Epoch 81/160, Validation Loss: 0.000241\n",
      "Epoch 82/160, Validation Loss: 0.000525\n",
      "Epoch 83/160, Validation Loss: 0.000362\n",
      "Epoch 84/160, Validation Loss: 0.000267\n",
      "Epoch 85/160, Validation Loss: 0.000460\n",
      "Epoch 86/160, Validation Loss: 0.000383\n",
      "Epoch 87/160, Validation Loss: 0.000285\n",
      "Epoch 88/160, Validation Loss: 0.000179\n",
      "Epoch 89/160, Validation Loss: 0.000727\n",
      "Epoch 90/160, Validation Loss: 0.000218\n",
      "Epoch 91/160, Validation Loss: 0.000293\n",
      "Epoch 92/160, Validation Loss: 0.000237\n",
      "Epoch 93/160, Validation Loss: 0.000295\n",
      "Epoch 94/160, Validation Loss: 0.000278\n",
      "Epoch 95/160, Validation Loss: 0.000244\n",
      "Epoch 96/160, Validation Loss: 0.000380\n",
      "Epoch 97/160, Validation Loss: 0.000240\n",
      "Epoch 98/160, Validation Loss: 0.000203\n",
      "Epoch 99/160, Validation Loss: 0.000458\n",
      "Epoch 100/160, Validation Loss: 0.001142\n",
      "Epoch 101/160, Validation Loss: 0.000186\n",
      "Epoch 102/160, Validation Loss: 0.000210\n",
      "Epoch 103/160, Validation Loss: 0.000204\n",
      "Epoch 104/160, Validation Loss: 0.000837\n",
      "Epoch 105/160, Validation Loss: 0.000248\n",
      "Epoch 106/160, Validation Loss: 0.000169\n",
      "Epoch 107/160, Validation Loss: 0.000162\n",
      "Epoch 108/160, Validation Loss: 0.000437\n",
      "Epoch 109/160, Validation Loss: 0.000180\n",
      "Epoch 110/160, Validation Loss: 0.000248\n",
      "Epoch 111/160, Validation Loss: 0.000201\n",
      "Epoch 112/160, Validation Loss: 0.000172\n",
      "Epoch 113/160, Validation Loss: 0.000210\n",
      "Epoch 114/160, Validation Loss: 0.000319\n",
      "Epoch 115/160, Validation Loss: 0.001229\n",
      "Epoch 116/160, Validation Loss: 0.000402\n",
      "Epoch 117/160, Validation Loss: 0.000175\n",
      "Epoch 118/160, Validation Loss: 0.000206\n",
      "Epoch 119/160, Validation Loss: 0.000295\n",
      "Epoch 120/160, Validation Loss: 0.000391\n",
      "Epoch 121/160, Validation Loss: 0.000200\n",
      "Epoch 122/160, Validation Loss: 0.000341\n",
      "Epoch 123/160, Validation Loss: 0.000818\n",
      "Epoch 124/160, Validation Loss: 0.000269\n",
      "Epoch 125/160, Validation Loss: 0.000226\n",
      "Epoch 126/160, Validation Loss: 0.000280\n",
      "Epoch 127/160, Validation Loss: 0.000159\n",
      "Epoch 128/160, Validation Loss: 0.000172\n",
      "Epoch 129/160, Validation Loss: 0.000297\n",
      "Epoch 130/160, Validation Loss: 0.000169\n",
      "Epoch 131/160, Validation Loss: 0.000203\n",
      "Epoch 132/160, Validation Loss: 0.000167\n",
      "Epoch 133/160, Validation Loss: 0.000235\n",
      "Epoch 134/160, Validation Loss: 0.000182\n",
      "Epoch 135/160, Validation Loss: 0.000208\n",
      "Epoch 136/160, Validation Loss: 0.000250\n",
      "Epoch 137/160, Validation Loss: 0.000811\n",
      "Epoch 138/160, Validation Loss: 0.000270\n",
      "Epoch 139/160, Validation Loss: 0.000278\n",
      "Epoch 140/160, Validation Loss: 0.000181\n",
      "Epoch 141/160, Validation Loss: 0.000201\n",
      "Epoch 142/160, Validation Loss: 0.000225\n",
      "Epoch 143/160, Validation Loss: 0.000265\n",
      "Epoch 144/160, Validation Loss: 0.000339\n",
      "Epoch 145/160, Validation Loss: 0.000310\n",
      "Epoch 146/160, Validation Loss: 0.000575\n",
      "Epoch 147/160, Validation Loss: 0.000437\n",
      "Epoch 148/160, Validation Loss: 0.000218\n",
      "Epoch 149/160, Validation Loss: 0.000168\n",
      "Epoch 150/160, Validation Loss: 0.000186\n",
      "Epoch 151/160, Validation Loss: 0.000191\n",
      "Epoch 152/160, Validation Loss: 0.000515\n",
      "Epoch 153/160, Validation Loss: 0.000191\n",
      "Epoch 154/160, Validation Loss: 0.000431\n",
      "Epoch 155/160, Validation Loss: 0.000186\n",
      "Epoch 156/160, Validation Loss: 0.000784\n",
      "Epoch 157/160, Validation Loss: 0.000194\n",
      "Epoch 158/160, Validation Loss: 0.000534\n",
      "Epoch 159/160, Validation Loss: 0.000163\n",
      "Epoch 160/160, Validation Loss: 0.000163\n",
      "Best Configuration: [60, 60, 60, 60, 60, 60]\n",
      "Validation Loss: 0.000157\n"
     ]
    }
   ],
   "source": [
    "# Grid search over number of layers and neurons\n",
    "layer_counts = [4, 5, 6, 7, 8, 9, 10]\n",
    "neuron_counts = [40, 50, 60, 70, 80]\n",
    "dropout_rate = 0\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_config = None\n",
    "best_model_weights = None\n",
    "\n",
    "for num_layers, num_neurons in product(layer_counts, neuron_counts):\n",
    "    hidden_sizes = [num_neurons] * num_layers\n",
    "    print(f\"Training model with {num_layers} layers and {num_neurons} neurons per layer...\")\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = RegressionNN(input_size=X_train.shape[1], hidden_sizes=hidden_sizes, dropout_rate=dropout_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    criterion2 = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 150\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                predictions = model(X_batch)\n",
    "                loss = criterion(predictions, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss:.6f}\")\n",
    "        \n",
    "        # Save the best weights for this configuration\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_config = hidden_sizes\n",
    "            best_model_weights = model.state_dict()\n",
    "            # Save weights\n",
    "            os.makedirs(os.path.dirname(weights_path), exist_ok=True)\n",
    "            torch.save(best_model_weights, weights_path)\n",
    "\n",
    "# Save the best configuration\n",
    "os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(f\"Best Configuration: {best_model_config}\\nValidation Loss: {best_val_loss:.6f}\")\n",
    "\n",
    "print(f\"Best Configuration: {best_model_config}\")\n",
    "print(f\"Validation Loss: {best_val_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b8acf53-68a5-4043-8579-5367cf4dcecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Configuration Details:\n",
      "Best Configuration: [60, 60, 60, 60, 60, 60]\n",
      "Validation Loss: 0.000157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19060\\3127276678.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model.load_state_dict(torch.load(weights_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.000157  Test Absolute Loss: 0.009745 \n"
     ]
    }
   ],
   "source": [
    "# Load the best model and weights for testing\n",
    "with open(config_path, 'r') as f:\n",
    "    config_details = f.read()\n",
    "print(\"Loaded Configuration Details:\")\n",
    "print(config_details)\n",
    "\n",
    "# Initialize the model with the best configuration\n",
    "best_model = RegressionNN(input_size=X_train.shape[1], hidden_sizes=best_model_config, dropout_rate=dropout_rate)\n",
    "best_model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_model.eval()\n",
    "test_loss = 0\n",
    "abs_test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        predictions = best_model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        abs_loss = criterion2(predictions, y_batch)\n",
    "        test_loss += loss.item()\n",
    "        abs_test_loss += abs_loss.item()\n",
    "test_loss /= len(test_loader)\n",
    "abs_test_loss /= len(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.6f}  Test Absolute Loss: {abs_test_loss:.6f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20a4577-7ba8-49f4-8190-ba2ae3da38d6",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9be1601-505c-47ec-a8a6-0d7b04fb84a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:-10].values\n",
    "y = df['Growth'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfec50eb-4cb6-4445-8999-1b94a892c384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e465dcc2-3a80-429e-9625-36669385ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X2, y_train, y2 = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X2, y2, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4964534-b15f-421f-a485-6887e40c0338",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}\n",
      "Validation MSE: 0.000664\n",
      "Testing parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 150}\n",
      "Validation MSE: 0.000601\n",
      "Testing parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
      "Validation MSE: 0.000572\n",
      "Testing parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300}\n",
      "Validation MSE: 0.000551\n",
      "Testing parameters: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100}\n",
      "Validation MSE: 0.000599\n",
      "Testing parameters: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150}\n",
      "Validation MSE: 0.000560\n",
      "Testing parameters: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200}\n",
      "Validation MSE: 0.000551\n",
      "Testing parameters: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 300}\n",
      "Validation MSE: 0.000549\n",
      "Testing parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100}\n",
      "Validation MSE: 0.000579\n",
      "Testing parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 150}\n",
      "Validation MSE: 0.000562\n",
      "Testing parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}\n",
      "Validation MSE: 0.000560\n",
      "Testing parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 300}\n",
      "Validation MSE: 0.000566\n",
      "Testing parameters: {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 100}\n",
      "Validation MSE: 0.000576\n",
      "Testing parameters: {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 150}\n",
      "Validation MSE: 0.000573\n",
      "Testing parameters: {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 200}\n",
      "Validation MSE: 0.000575\n",
      "Testing parameters: {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 300}\n",
      "Validation MSE: 0.000586\n",
      "Testing parameters: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 100}\n",
      "Validation MSE: 0.000580\n",
      "Testing parameters: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 150}\n",
      "Validation MSE: 0.000563\n",
      "Testing parameters: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 200}\n",
      "Validation MSE: 0.000560\n",
      "Testing parameters: {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 300}\n",
      "Validation MSE: 0.000559\n",
      "Testing parameters: {'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 100}\n",
      "Validation MSE: 0.000566\n",
      "Testing parameters: {'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 150}\n",
      "Validation MSE: 0.000566\n",
      "Testing parameters: {'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 200}\n",
      "Validation MSE: 0.000570\n",
      "Testing parameters: {'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 300}\n",
      "Validation MSE: 0.000583\n",
      "Testing parameters: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 100}\n",
      "Validation MSE: 0.000570\n",
      "Testing parameters: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 150}\n",
      "Validation MSE: 0.000578\n",
      "Testing parameters: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 200}\n",
      "Validation MSE: 0.000588\n",
      "Testing parameters: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 300}\n",
      "Validation MSE: 0.000608\n",
      "Testing parameters: {'learning_rate': 0.2, 'max_depth': 8, 'n_estimators': 100}\n",
      "Validation MSE: 0.000594\n",
      "Testing parameters: {'learning_rate': 0.2, 'max_depth': 8, 'n_estimators': 150}\n",
      "Validation MSE: 0.000607\n",
      "Testing parameters: {'learning_rate': 0.2, 'max_depth': 8, 'n_estimators': 200}\n",
      "Validation MSE: 0.000619\n",
      "Testing parameters: {'learning_rate': 0.2, 'max_depth': 8, 'n_estimators': 300}\n",
      "Validation MSE: 0.000645\n",
      "Testing parameters: {'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 100}\n",
      "Validation MSE: 0.000566\n",
      "Testing parameters: {'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 150}\n",
      "Validation MSE: 0.000559\n",
      "Testing parameters: {'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 200}\n",
      "Validation MSE: 0.000561\n",
      "Testing parameters: {'learning_rate': 0.3, 'max_depth': 5, 'n_estimators': 300}\n",
      "Validation MSE: 0.000570\n",
      "Testing parameters: {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 100}\n",
      "Validation MSE: 0.000578\n",
      "Testing parameters: {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 150}\n",
      "Validation MSE: 0.000583\n",
      "Testing parameters: {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 200}\n",
      "Validation MSE: 0.000595\n",
      "Testing parameters: {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 300}\n",
      "Validation MSE: 0.000619\n",
      "Testing parameters: {'learning_rate': 0.3, 'max_depth': 7, 'n_estimators': 100}\n",
      "Validation MSE: 0.000599\n",
      "Testing parameters: {'learning_rate': 0.3, 'max_depth': 7, 'n_estimators': 150}\n",
      "Validation MSE: 0.000614\n",
      "Testing parameters: {'learning_rate': 0.3, 'max_depth': 7, 'n_estimators': 200}\n",
      "Validation MSE: 0.000628\n",
      "Testing parameters: {'learning_rate': 0.3, 'max_depth': 7, 'n_estimators': 300}\n",
      "Validation MSE: 0.000653\n",
      "Testing parameters: {'learning_rate': 0.3, 'max_depth': 8, 'n_estimators': 100}\n",
      "Validation MSE: 0.000637\n",
      "Testing parameters: {'learning_rate': 0.3, 'max_depth': 8, 'n_estimators': 150}\n",
      "Validation MSE: 0.000662\n",
      "Testing parameters: {'learning_rate': 0.3, 'max_depth': 8, 'n_estimators': 200}\n",
      "Validation MSE: 0.000680\n",
      "Testing parameters: {'learning_rate': 0.3, 'max_depth': 8, 'n_estimators': 300}\n",
      "Validation MSE: 0.000697\n",
      "\n",
      "Best Parameters:\n",
      "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 300}\n",
      "Lowest Validation MSE: 0.000549\n",
      "\n",
      "Test MSE: 0.000542\n",
      "Test MAE: 0.018528\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [5, 6, 7, 8],\n",
    "    'learning_rate': [0.1, 0.2, 0.3],\n",
    "    'n_estimators': [100, 150, 200, 300],\n",
    "}\n",
    "\n",
    "# Grid Search over hyperparameters\n",
    "best_params = None\n",
    "lowest_val_mse = float(\"inf\")\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Testing parameters: {params}\")\n",
    "    \n",
    "    # Train the model with the current parameters\n",
    "    model = xgb.XGBRegressor(\n",
    "        max_depth=params['max_depth'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        n_estimators=params['n_estimators'],\n",
    "        random_state=1\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        #early_stopping_rounds=10,  # Stop early if no improvement\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Calculate validation MSE\n",
    "    val_predictions = model.predict(X_val)\n",
    "    val_mse = mean_squared_error(y_val, val_predictions)\n",
    "    print(f\"Validation MSE: {val_mse:.6f}\")\n",
    "    \n",
    "    # Check if this is the best model so far\n",
    "    if val_mse < lowest_val_mse:\n",
    "        lowest_val_mse = val_mse\n",
    "        best_params = params\n",
    "        best_model = model\n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(best_params)\n",
    "print(f\"Lowest Validation MSE: {lowest_val_mse:.6f}\")\n",
    "\n",
    "# Step 4: Evaluate the best model on the test set\n",
    "test_predictions = best_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "\n",
    "print(f\"\\nTest MSE: {test_mse:.6f}\")\n",
    "print(f\"Test MAE: {test_mae:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bf9766c-f9a3-4442-94f3-8c885acd1e0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=300, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;XGBRegressor<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=300, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=300, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "\n",
    "\n",
    "X = df.iloc[:, 1:-10]\n",
    "y = df['Growth']\n",
    "\n",
    "model = XGBRegressor(max_depth=6, n_estimators=300, learning_rate=0.1)\n",
    "model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14b24775-46bf-4a56-ad3a-ec669f3cf498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Feature importance'}>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_important = model.get_booster().get_score(importance_type='weight')\n",
    "keys = list(feature_important.keys())\n",
    "values = list(feature_important.values())\n",
    "\n",
    "data = pd.DataFrame(data=values, index=keys, columns=[\"F-score\"]).sort_values(by = \"F-score\", ascending=True)\n",
    "data.nlargest(15, columns=\"F-score\").plot(kind='barh', figsize = (10,5), title=\"Feature importance\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdf84da-2834-450b-b66a-d1278d4eaf51",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c78e8a7-6ca6-40a2-b525-a51080338cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "X_train, X2, y_train, y2 = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X2, y2, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42a3482f-8f5d-4bae-8fea-073ee8209c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Validation MSE: 0.000370\n",
      "Testing parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Validation MSE: 0.000369\n",
      "Testing parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 500}\n",
      "Validation MSE: 0.000369\n",
      "Testing parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_split': 4, 'n_estimators': 100}\n",
      "Validation MSE: 0.000376\n",
      "Testing parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_split': 4, 'n_estimators': 200}\n",
      "Validation MSE: 0.000376\n",
      "Testing parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_split': 4, 'n_estimators': 500}\n",
      "Validation MSE: 0.000376\n",
      "Testing parameters: {'max_depth': 50, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Validation MSE: 0.000374\n",
      "Testing parameters: {'max_depth': 50, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Validation MSE: 0.000370\n",
      "Testing parameters: {'max_depth': 50, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 500}\n",
      "Validation MSE: 0.000369\n",
      "Testing parameters: {'max_depth': 50, 'max_features': 'sqrt', 'min_samples_split': 4, 'n_estimators': 100}\n",
      "Validation MSE: 0.000374\n",
      "Testing parameters: {'max_depth': 50, 'max_features': 'sqrt', 'min_samples_split': 4, 'n_estimators': 200}\n",
      "Validation MSE: 0.000374\n",
      "Testing parameters: {'max_depth': 50, 'max_features': 'sqrt', 'min_samples_split': 4, 'n_estimators': 500}\n",
      "Validation MSE: 0.000374\n",
      "Testing parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Validation MSE: 0.000374\n",
      "Testing parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Validation MSE: 0.000370\n",
      "Testing parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 500}\n",
      "Validation MSE: 0.000369\n",
      "Testing parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_split': 4, 'n_estimators': 100}\n",
      "Validation MSE: 0.000374\n",
      "Testing parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_split': 4, 'n_estimators': 200}\n",
      "Validation MSE: 0.000374\n",
      "Testing parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_split': 4, 'n_estimators': 500}\n",
      "Validation MSE: 0.000374\n",
      "\n",
      "Best Parameters:\n",
      "{'max_depth': 20, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 500}\n",
      "Lowest Validation MSE: 0.000369\n",
      "\n",
      "Test MSE: 0.000366\n",
      "Test MAE: 0.015211\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [20, 50, None],\n",
    "    'min_samples_split': [2, 4],\n",
    "    #'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt']  # Corrected parameter options\n",
    "}\n",
    "\n",
    "# Step 3: Grid Search over hyperparameters\n",
    "best_params = None\n",
    "lowest_val_mse = float(\"inf\")\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Testing parameters: {params}\")\n",
    "    \n",
    "    # Train the model with the current parameters\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=params['n_estimators'],\n",
    "        max_depth=params['max_depth'],\n",
    "        min_samples_split=params['min_samples_split'],\n",
    "        #min_samples_leaf=params['min_samples_leaf'],\n",
    "        max_features=params['max_features'],\n",
    "        random_state=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate validation MSE\n",
    "    val_predictions = model.predict(X_val)\n",
    "    val_mse = mean_squared_error(y_val, val_predictions)\n",
    "    print(f\"Validation MSE: {val_mse:.6f}\")\n",
    "    \n",
    "    # Check if this is the best model so far\n",
    "    if val_mse < lowest_val_mse:\n",
    "        lowest_val_mse = val_mse\n",
    "        best_params = params\n",
    "        best_model = model\n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(best_params)\n",
    "print(f\"Lowest Validation MSE: {lowest_val_mse:.6f}\")\n",
    "\n",
    "# Step 4: Evaluate the best model on the test set\n",
    "test_predictions = best_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "\n",
    "print(f\"\\nTest MSE: {test_mse:.6f}\")\n",
    "print(f\"Test MAE: {test_mae:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bf1395-82c4-4f1d-8b45-fa03ab09992c",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1d18b4ba-545a-4db9-81a7-d973f55d4bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Pi'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c9559ce7-f0bd-406d-b94d-06871e694b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "X_train, X2, y_train, y2 = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X2, y2, test_size=0.5, random_state=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "75fa9358-fb97-448d-b23e-3b22a9637e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters: {'C': 3, 'epsilon': 0.01, 'kernel': 'linear'}\n",
      "Validation MSE: 770146639.007987\n",
      "Testing parameters: {'C': 3, 'epsilon': 0.01, 'kernel': 'rbf'}\n",
      "Validation MSE: 4044376364.132330\n",
      "\n",
      "Best Parameters:\n",
      "{'C': 3, 'epsilon': 0.01, 'kernel': 'linear'}\n",
      "Lowest Validation MSE: 770146639.007987\n",
      "\n",
      "Test MSE: 781001169.766642\n",
      "Test MAE: 19589.900949\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 0.5, 1, 3],              # Regularization parameter\n",
    "    'epsilon': [0.01, 0.1, 0.5 ],      # Epsilon in the epsilon-SVR model\n",
    "    'kernel': ['linear', 'rbf']     \n",
    "}\n",
    "\n",
    "# Step 4: Grid Search over hyperparameters\n",
    "best_params = None\n",
    "lowest_val_mse = float(\"inf\")\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Testing parameters: {params}\")\n",
    "    \n",
    "    # Train the model with the current parameters\n",
    "    model = SVR(\n",
    "        C=params['C'],\n",
    "        epsilon=params['epsilon'],\n",
    "        kernel=params['kernel']\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate validation MSE\n",
    "    val_predictions = model.predict(X_val)\n",
    "    val_mse = mean_squared_error(y_val, val_predictions)\n",
    "    print(f\"Validation MSE: {val_mse:.6f}\")\n",
    "    \n",
    "    # Check if this is the best model so far\n",
    "    if val_mse < lowest_val_mse:\n",
    "        lowest_val_mse = val_mse\n",
    "        best_params = params\n",
    "        best_model = model\n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(best_params)\n",
    "print(f\"Lowest Validation MSE: {lowest_val_mse:.6f}\")\n",
    "\n",
    "# Step 5: Evaluate the best model on the test set\n",
    "test_predictions = best_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "\n",
    "print(f\"\\nTest MSE: {test_mse:.6f}\")\n",
    "print(f\"Test MAE: {test_mae:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d802e6c3-b9b2-458c-9977-3ae7459c42e6",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b552ecb8-6b2c-47d5-bca2-72f93ada5579",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:-10].values\n",
    "y = df['Pi'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81e48b59-7610-43f9-aba1-1e1aebf457d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "X_train, X2, y_train, y2 = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X2, y2, test_size=0.5, random_state=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9ec3528-745a-4323-bff3-a85ba3271373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters: {'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE: 673843955.441487\n",
      "Testing parameters: {'n_neighbors': 3, 'p': 1, 'weights': 'distance'}\n",
      "Validation MSE: 693011583.422729\n",
      "Testing parameters: {'n_neighbors': 3, 'p': 2, 'weights': 'uniform'}\n",
      "Validation MSE: 671975644.124603\n",
      "Testing parameters: {'n_neighbors': 3, 'p': 2, 'weights': 'distance'}\n",
      "Validation MSE: 682014506.515200\n",
      "Testing parameters: {'n_neighbors': 4, 'p': 1, 'weights': 'uniform'}\n",
      "Validation MSE: 643756421.117599\n",
      "Testing parameters: {'n_neighbors': 4, 'p': 1, 'weights': 'distance'}\n",
      "Validation MSE: 654672375.988829\n",
      "Testing parameters: {'n_neighbors': 4, 'p': 2, 'weights': 'uniform'}\n",
      "Validation MSE: 642595362.946812\n",
      "Testing parameters: {'n_neighbors': 4, 'p': 2, 'weights': 'distance'}\n",
      "Validation MSE: 644789677.561418\n",
      "Testing parameters: {'n_neighbors': 5, 'p': 1, 'weights': 'uniform'}\n",
      "Validation MSE: 634858736.495756\n",
      "Testing parameters: {'n_neighbors': 5, 'p': 1, 'weights': 'distance'}\n",
      "Validation MSE: 637944970.250759\n",
      "Testing parameters: {'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n",
      "Validation MSE: 635050069.543285\n",
      "Testing parameters: {'n_neighbors': 5, 'p': 2, 'weights': 'distance'}\n",
      "Validation MSE: 627668217.330856\n",
      "Testing parameters: {'n_neighbors': 6, 'p': 1, 'weights': 'uniform'}\n",
      "Validation MSE: 627065204.810705\n",
      "Testing parameters: {'n_neighbors': 6, 'p': 1, 'weights': 'distance'}\n",
      "Validation MSE: 624967550.886667\n",
      "Testing parameters: {'n_neighbors': 6, 'p': 2, 'weights': 'uniform'}\n",
      "Validation MSE: 632270867.841326\n",
      "Testing parameters: {'n_neighbors': 6, 'p': 2, 'weights': 'distance'}\n",
      "Validation MSE: 617805978.063957\n",
      "Testing parameters: {'n_neighbors': 7, 'p': 1, 'weights': 'uniform'}\n",
      "Validation MSE: 623238211.685816\n",
      "Testing parameters: {'n_neighbors': 7, 'p': 1, 'weights': 'distance'}\n",
      "Validation MSE: 616452175.469402\n",
      "Testing parameters: {'n_neighbors': 7, 'p': 2, 'weights': 'uniform'}\n",
      "Validation MSE: 626074247.749658\n",
      "Testing parameters: {'n_neighbors': 7, 'p': 2, 'weights': 'distance'}\n",
      "Validation MSE: 606735131.842623\n",
      "Testing parameters: {'n_neighbors': 10, 'p': 1, 'weights': 'uniform'}\n",
      "Validation MSE: 617483250.867441\n",
      "Testing parameters: {'n_neighbors': 10, 'p': 1, 'weights': 'distance'}\n",
      "Validation MSE: 604422656.863023\n",
      "Testing parameters: {'n_neighbors': 10, 'p': 2, 'weights': 'uniform'}\n",
      "Validation MSE: 623354678.545516\n",
      "Testing parameters: {'n_neighbors': 10, 'p': 2, 'weights': 'distance'}\n",
      "Validation MSE: 595284383.378279\n",
      "\n",
      "Best Parameters:\n",
      "{'n_neighbors': 10, 'p': 2, 'weights': 'distance'}\n",
      "Lowest Validation MSE: 595284383.378279\n",
      "\n",
      "Test MSE: 583284348.064226\n",
      "Test MAE: 17726.403024\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [3, 4, 5, 6, 7, 10],\n",
    "    'p': [1, 2],  # p=1 for Manhattan distance, p=2 for Euclidean distance\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "# Step 3: Grid Search over hyperparameters\n",
    "best_params = None\n",
    "lowest_val_mse = float(\"inf\")\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Testing parameters: {params}\")\n",
    "    \n",
    "    # Train the model with the current parameters\n",
    "    model = KNeighborsRegressor(\n",
    "        n_neighbors=params['n_neighbors'],\n",
    "        p=params['p'],\n",
    "        weights=params['weights']\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate validation MSE\n",
    "    val_predictions = model.predict(X_val)\n",
    "    val_mse = mean_squared_error(y_val, val_predictions)\n",
    "    print(f\"Validation MSE: {val_mse:.6f}\")\n",
    "    \n",
    "    # Check if this is the best model so far\n",
    "    if val_mse < lowest_val_mse:\n",
    "        lowest_val_mse = val_mse\n",
    "        best_params = params\n",
    "        best_model = model\n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(best_params)\n",
    "print(f\"Lowest Validation MSE: {lowest_val_mse:.6f}\")\n",
    "\n",
    "# Step 4: Evaluate the best model on the test set\n",
    "test_predictions = best_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "\n",
    "print(f\"\\nTest MSE: {test_mse:.6f}\")\n",
    "print(f\"Test MAE: {test_mae:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
